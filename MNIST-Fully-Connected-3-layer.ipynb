{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at index 3: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "y = np.matrix(digits.target).T\n",
    "X = np.matrix(digits.data)\n",
    "print(\"Image at index 3: \")\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[3]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of data:  ['data', 'target', 'target_names', 'images', 'DESCR']\n",
      "Label of an image:  3\n"
     ]
    }
   ],
   "source": [
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "# N\n",
    "# list(digits.data[0].shape)\n",
    "# X\n",
    "attributes = list(digits)\n",
    "print(\"Attributes of data: \", attributes)\n",
    "print(\"Label of an image: \", digits.target[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to one hot encode the outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 16\n",
    "h1 = 32\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "L = len(W)-1\n",
    "print(L)\n",
    "# def act(z):\n",
    "#     return z if z>0 else 0\n",
    "# def actder(z):\n",
    "#     return 1 if z>0 else 0\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_der(x):\n",
    "#     print(x.shape)\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_grad(x):\n",
    "    prob = softmax(x)\n",
    "    return np.multiply(prob, (1-prob))\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "#     s = softmax.reshape(-1,1)\n",
    "#     return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "    m = y.shape[0]\n",
    "    grad = softmax(X)\n",
    "#     grad = X\n",
    "    idx = np.where(grad == y*grad)\n",
    "    grad[idx] = grad[idx] - 1\n",
    "    grad = grad/m\n",
    "    return grad\n",
    "\n",
    "# def softmax(X)\n",
    "# y = np.matrix([1,0,2])\n",
    "# X = np.matrix([2,4,1])\n",
    "# print(delta_cross_entropy(y.T,X))\n",
    "# print(softmax(np.array([1,2,3])))\n",
    "# img = images[img_idx]\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return y-yhat \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1) and (10,1) not aligned: 1 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c423aecef4e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#         print(\"-----------------------------------------------------------\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#         print(\"Backpropogation\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#         print(delta[layer])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a91385545fe8>\u001b[0m in \u001b[0;36mdelta_cross_entropy\u001b[0;34m(y, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#     grad = X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,1) and (10,1) not aligned: 1 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Use mini-batch size 1\n",
    "\n",
    "alpha = 0.01\n",
    "#originally max-iter = 1000\n",
    "max_iter = 5\n",
    "\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = X[order[i],:].T\n",
    "        y_this = y[order[i],0]\n",
    "        yt = np.matrix([np.zeros(10)]).T\n",
    "#         print(yt.shape, y_this)\n",
    "        yt[y_this] = 1\n",
    "#         print(yt)\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "#         for l in range(1,L+1):\n",
    "#             print(\"a-l: \", l-1,\"  actual l: \", l)\n",
    "        layer =1\n",
    "        #First layer\n",
    "        z.append(W[layer].T*a[layer-1]+b[layer])\n",
    "        a.append(relu(z[-1]))\n",
    "        delta.append([]); dW.append([]); db.append([])\n",
    "#         print(\"z shape at end of 1st layer:\", z[-1].shape)        \n",
    "#         print(\"a shape at end of 1st layer:\", a[-1].shape)\n",
    "\n",
    "        layer=2\n",
    "        #Second layer\n",
    "        z.append(W[layer].T*a[layer-1]+b[layer])\n",
    "        a.append(relu(z[-1]))\n",
    "        delta.append([]); dW.append([]); db.append([])\n",
    "        \n",
    "#         print(\"z shape at end of 2nd layer:\", z[-1].shape)        \n",
    "#         print(\"a shape at end of 2nd layer:\", a[-1].shape)\n",
    "\n",
    "        layer=3\n",
    "        #third layer\n",
    "        z.append(W[layer].T*a[layer-1]+b[layer])\n",
    "        a.append(softmax(z[-1]))\n",
    "        delta.append([]); dW.append([]); db.append([])\n",
    "        \n",
    "#         print(\"z shape at end of 3rd layer:\", z[-1].shape)\n",
    "#         print(a[-1].sum())\n",
    "        \n",
    "        loss_this_pattern = np.sum(np.subtract(yt, a[3]))\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "#         # Backprop step\n",
    "\n",
    "#         delta[layer] = np.subtract(yt, a[3])\n",
    "#         print(type(yt))\n",
    "   \n",
    "#         print(\"-----------------------------------------------------------\")\n",
    "#         print(\"Backpropogation\")\n",
    "        delta[3] = delta_cross_entropy(a[3], yt)\n",
    "#         print(delta[layer])\n",
    "        db[3] = delta[3].copy()\n",
    "        dW[3] = a[3-1] * delta[3].T\n",
    "#         print(dW[layer].shape)\n",
    "        delta[3-1] = np.multiply(softmax_grad(z[3-1]), W[3] * delta[3])\n",
    "#         print(delta[layer].shape)\n",
    "#         print(delta[layer-1].shape)\n",
    "#         print(dW[layer].shape)\n",
    "#         print(softmax_grad(z[layer-1]).shape)\n",
    "        \n",
    "        layer=2\n",
    "        db[2] = delta[2].copy()\n",
    "        dW[2] = a[2-1] * delta[2].T\n",
    "        delta[2-1] = np.multiply(relu_der(z[2-1]), W[2] * delta[2])\n",
    "        \n",
    "        layer=1\n",
    "        db[1] = delta[1].copy()\n",
    "        dW[1] = a[1-1] * delta[1].T\n",
    "#         delta[layer-1] = np.multiply(relu_der(z[layer-1]), W[layer] * delta[layer].T)\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))\n",
    "    \n",
    "    \n",
    "    print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
