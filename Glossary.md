# Glossary

## Loss function
It is a performance metric on how well the Neural Network manages to reach its goal of generation oupts as close as possible to desired values. 

Example:  <br />
 **loss** = Desired output - Actual output;
 But this is schoolboy stuff, we will most likely work with Sum of squars errors which is notably the most famous loss function in Neural Network.

 <br />
 **Absolute error**: It is the result to MOD function applied to an error with a sign or direction.
  <br />
 As a summary, the loss function is an error metric, that gives an indicator on how much precision we lose, if we replace the real desired output by the actual output generated by our trained neural network model. That's why it's called **loss**!

 ## Derivative of error

Our main goal is to optimize the error, and make it as less as possible. There are different techniques and algorithms which adjusts the weights. Since we are dealing with images, we will avoid brute force technique. So one of powerful concept to deal with errors in finding the rate at which error is changing its value at this point ie derivative.
    Derivative of loss function is directly applied on weight(s). 
 
 ### Then how are weights adjusted?
 Let OW be the Optimal weight.
     - If w<OW, we have a positive loss function, but the derivative is negative, meaning that an increase of weight will decrease the loss function.
    - At w=OW, the loss is 0 and the derivative is 0, we reached a perfect model, nothing is needed.
    - If w>OW, the loss becomes positive again, but the derivative is as well positive, meaning that any more increase in the weight, will increase the losses even more!!
    
 ## Convolution.  
    
