# Glossary

## Loss function
It is a performance metric on how well the Neural Network manages to reach its goal of generation oupts as close as possible to desired values. 

Example:  <br />
 **loss** = Desired output - Actual output;
 But this is schoolboy stuff, we will most likely work with Sum of squars errors which is notably the most famous loss function in Neural Network.

 <br />
 **Absolute error**: It is the result to MOD function applied to an error with a sign or direction.
  <br />
 As a summary, the loss function is an error metric, that gives an indicator on how much precision we lose, if we replace the real desired output by the actual output generated by our trained neural network model. That's why it's called **loss**!

 ## Derivative of error

Our main goal is to optimize the error, and make it as less as possible. There are different techniques and algorithms which adjusts the weights. Since we are dealing with images, we will avoid brute force technique. So one of powerful concept to deal with errors in finding the rate at which error is changing its value at this point ie derivative.
    Derivative of loss function is directly applied on weight(s). 
 
 ### Then how are weights adjusted?
 Let OW be the Optimal weight.
     - If w<OW, we have a positive loss function, but the derivative is negative, meaning that an increase of weight will decrease the loss function.
    - At w=OW, the loss is 0 and the derivative is 0, we reached a perfect model, nothing is needed.
    - If w>OW, the loss becomes positive again, but the derivative is as well positive, meaning that any more increase in the weight, will increase the losses even more!!
    
 ## Convolution.  
      ___ to be decided ___
      

 ## Pooling
 
 We can think of max-pooling as a way for the network to ask whether a given feature is found anywhere in a region of the image. It then throws away the exact positional information. The intuition is that once a feature has been found, its exact location isn't as important as its rough location relative to other features. A big benefit is that there are many fewer pooled features, and so this helps reduce the number of parameters needed in later layers.

Max-pooling isn't the only technique used for pooling. Another common approach is known as L2 pooling. Here, instead of taking the maximum activation of a 2×22×2 region of neurons, we take the square root of the sum of the squares of the activations in the 2×22×2 region. While the details are different, the intuition is similar to max-pooling: L2 pooling is a way of condensing information from the convolutional layer. In practice, both techniques have been widely used. And sometimes people use other types of pooling operation. If you're really trying to optimize performance, you may use validation data to compare several different approaches to pooling, and choose the approach which works best. 
