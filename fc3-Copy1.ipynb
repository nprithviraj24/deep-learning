{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at index 3: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of data:  ['data', 'target', 'target_names', 'images', 'DESCR']\n",
      "Label of an image:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "digits = load_digits()\n",
    "y = np.matrix(digits.target).T\n",
    "X = np.matrix(digits.data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "M = X_train.shape[0]\n",
    "N = X_train.shape[1]\n",
    "\n",
    "print(\"Image at index 3: \")\n",
    "import matplotlib.pyplot as plt \n",
    "plt.matshow(digits.images[3]) \n",
    "plt.show()\n",
    "\n",
    "attributes = list(digits)\n",
    "print(\"Attributes of data: \", attributes)\n",
    "print(\"Label of an image: \", digits.target[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 3289.703119\n",
      "Iteration 1 loss 3225.179917\n",
      "Iteration 2 loss 3162.398471\n",
      "Iteration 3 loss 3101.183751\n",
      "Iteration 4 loss 3040.851257\n",
      "Iteration 5 loss 2982.165804\n",
      "Iteration 6 loss 2924.045024\n",
      "Iteration 7 loss 2866.166180\n",
      "Iteration 8 loss 2809.678418\n",
      "Iteration 9 loss 2751.950896\n",
      "Iteration 10 loss 2697.190427\n",
      "Iteration 11 loss 2639.215862\n",
      "Iteration 12 loss 2585.012645\n",
      "Iteration 13 loss 2528.812201\n",
      "Iteration 14 loss 2473.317684\n",
      "Iteration 15 loss 2416.649598\n",
      "Iteration 16 loss 2362.401725\n",
      "Iteration 17 loss 2309.566628\n",
      "Iteration 18 loss 2248.605079\n",
      "Iteration 19 loss 2194.034273\n",
      "Iteration 20 loss 2141.356123\n",
      "Iteration 21 loss 2082.210345\n",
      "Iteration 22 loss 2030.351789\n",
      "Iteration 23 loss 1979.032886\n",
      "Iteration 24 loss 1930.661853\n",
      "Iteration 25 loss 1868.840747\n",
      "Iteration 26 loss 1821.553000\n",
      "Iteration 27 loss 1766.103359\n",
      "Iteration 28 loss 1720.403307\n",
      "Iteration 29 loss 1671.600773\n",
      "Iteration 30 loss 1620.293514\n",
      "Iteration 31 loss 1576.327379\n",
      "Iteration 32 loss 1534.985076\n",
      "Iteration 33 loss 1507.756426\n",
      "Iteration 34 loss 1474.879499\n",
      "Iteration 35 loss 1436.423305\n",
      "Iteration 36 loss 1426.881174\n",
      "Iteration 37 loss 1392.658900\n",
      "Iteration 38 loss 1404.274800\n"
     ]
    }
   ],
   "source": [
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    return X/np.max(X)\n",
    "\n",
    "XX = normalize(X_train)\n",
    "Xtest = normalize(X_test)\n",
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "# h8 = 32\n",
    "# h7 = 64\n",
    "# h6 = 128\n",
    "# h5 = 256\n",
    "# h4 = 512\n",
    "# h3 = 100\n",
    "h2 = 100\n",
    "h1 = 200\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "#          np.random.normal(0,0.1,[h2,h3]),\n",
    "#          np.random.normal(0,0.1,[h3,h4]),\n",
    "#          np.random.normal(0,0.1,[h4,h5]),\n",
    "#          np.random.normal(0,0.1,[h5,h6]),\n",
    "#          np.random.normal(0,0.1,[h6,h7]),\n",
    "#          np.random.normal(0,0.1,[h7,h8]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "#          np.random.normal(0,0.1,[h3,1]),\n",
    "#          np.random.normal(0,0.1,[h4,1]),\n",
    "#          np.random.normal(0,0.1,[h5,1]),\n",
    "#          np.random.normal(0,0.1,[h6,1]),\n",
    "#          np.random.normal(0,0.1,[h7,1]),\n",
    "#          np.random.normal(0,0.1,[h8,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "L = len(W)-1\n",
    "\n",
    "def RELU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_der(z):\n",
    "    return 1.*(z>0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_der(x):\n",
    "    prob = softmax(x)\n",
    "#     print(np.shape(prob))\n",
    "    r = np.multiply(prob, (1-prob))\n",
    "    return r\n",
    "\n",
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual    \n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    l = np.log(ypred)\n",
    "    return np.sum(-1 * l.T * yactual)\n",
    "\n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y.T*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))\n",
    "    \n",
    "# Use mini-batch size 1\n",
    "# M = 6\n",
    "alpha = 0.005\n",
    "max_iter = 1000\n",
    "error = math.inf\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    prev_iter_loss = error\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y_train[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if l != L:\n",
    "                a.append(RELU(z[l]))\n",
    "            if l == L:\n",
    "                a.append(softmax(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "        \n",
    "        loss_this_pattern = crossEntropy( a[L], y_this, 10)\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step\n",
    "\n",
    "        delta[L] = delta_cross_entropy(oneHotEncode(y_this, 10), a[L])\n",
    "#         print(np.sum(delta[L]))\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                if l == L:\n",
    "#                     WxD = W[l]*delta[l]\n",
    "                    delta[l-1] = np.multiply(softmax_der(z[l-1]), W[l]*delta[l] )\n",
    "#                     print(\"softmax delt: \", np.sum(delta[l-1]))\n",
    "                else:\n",
    "                    delta[l-1] = np.multiply(relu_der(z[l-1]), W[l] * delta[l])\n",
    "#                     print(\"relu delt: \", np.sum(delta[l-1]))\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))\n",
    "    error = loss_this_iter\n",
    "    if loss_this_iter > prev_iter_loss:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6527777777777778\n"
     ]
    }
   ],
   "source": [
    "m = Xtest.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "truePositives = []\n",
    "for i in range(0,m):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = Xtest[order[i],:].T\n",
    "        y_this = y_test[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if l != L:\n",
    "                a.append(RELU(z[l]))\n",
    "            if l == L:\n",
    "                a.append(softmax(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "#         highest = np.where(a[L]>=0.5)\n",
    "        highest = np.argmax(a[L])\n",
    "#         print(np.argmax(a[L]))\n",
    "#         print(highest, \"--- \", y_this)\n",
    "        if highest == y_this:\n",
    "            truePositives.append(True)\n",
    "    \n",
    "accuracy = len(truePositives)/ Xtest.shape[0]\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper\n",
    "# img_idx = np.random.random_integers(digits.images.shape[0])\n",
    "# img = digits.images[img_idx]\n",
    "# for l in range(1,L+1):\n",
    "#     z.append(W[l].T*a[l-1]+b[l])\n",
    "#     if l != L:\n",
    "#         a.append(RELU(z[l]))\n",
    "#     if l == L:\n",
    "#         a.append(softmax(z[l]))\n",
    "    \n",
    "# # helper.view_classify(img.view( 8, 8), a[L])\n",
    "# import matplotlib.pyplot as plt\n",
    "# # ps = a[L].data.numpy().squeeze()\n",
    "# fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "# ax1.imshow(img.resize(1, 28, 28))\n",
    "# ax1.axis('off')\n",
    "# ax2.barh(np.arange(10), a[L])\n",
    "# ax2.set_aspect(0.1)\n",
    "# ax2.set_yticks(np.arange(10))\n",
    "# ax2.set_title('Class Probability')\n",
    "# ax2.set_xlim(0, 1.1)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
