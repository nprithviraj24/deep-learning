{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humpback Whale challenge\n",
    "\n",
    "Proposed solution to solve [this challenge](https://www.kaggle.com/c/humpback-whale-identification).\n",
    "\n",
    "Prithvi Raju [email](nprihviraj24@gmail.com)\n",
    "\n",
    "<strong>Objective<strong>: identify individual whales in images. \n",
    "    \n",
    "Constituents of dataset: \n",
    "\n",
    "    - train.zip - a folder containing the training images\n",
    "    - train.csv - maps the training Image to the appropriate whale Id. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.\n",
    "    - test.zip - a folder containing the test images to predict the whale Id\n",
    "    - sample_submission.csv - a sample submission file in the correct format\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the data\n",
    "\n",
    "Before building the model, lets understand the data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training samples:  (9850,)\n",
      " Number of unique whales with pictures more than one:  new_whale    810\n",
      "w_1287fbc     34\n",
      "w_98baff9     27\n",
      "w_7554f44     26\n",
      "w_1eafe46     23\n",
      "            ... \n",
      "w_80c692d      2\n",
      "w_eb44149      2\n",
      "w_73cbacd      2\n",
      "w_17a3581      2\n",
      "w_0466071      2\n",
      "Name: Id, Length: 2031, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('train.csv')\n",
    "ids = file.Id\n",
    "print(\" Number of training samples: \", ids.shape)\n",
    "uni = file.Id.value_counts()\n",
    "gt1 = uni[uni>1]\n",
    "print(\" Number of unique whales with pictures more than one: \", gt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train folder:  9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Image    000466c4.jpg\n",
       "Id          w_1287fbc\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"train/train\"))\n",
    "file_count = len(files)\n",
    "print(\"Number of images in train folder: \", file_count)\n",
    "\n",
    "file.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that there are 4251 classes for only 9850 images. Most of the \"class\" have only one training image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning using CNN block from Siamese network and  Batch hard strategy for optimization\n",
    "\n",
    "<br />\n",
    "\n",
    "<h2> Pipeline to create a maching network<h2>\n",
    "\n",
    ">  Data preprocessing and augmentation \n",
    "\n",
    "    Through this post from he has done data preprocessing steps\n",
    "\n",
    ">  Matching Network\n",
    "    \n",
    "    A method used to represent discrete variables as continuous vectors.\n",
    "   \n",
    ">> Build an Encoder Network  \n",
    "\n",
    ">> Generate \"image\" embeddings \n",
    "\n",
    ">> Pairwise distance between query samples and support sets.\n",
    "\n",
    ">> Calculating predictions by taking weighted average of the support set labels with the normalised distance.\n",
    "\n",
    "\n",
    ">  Batch hard strategy for addressing loss functions \n",
    "\n",
    "            By using Online triplet mining.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model, I incorporate the ResNet block, wonderfully explained in this [blog](http://teleported.in/posts/decoding-resnet-architecture/) post. \n",
    "\n",
    "Why ResNet?\n",
    "\n",
    "The rule of thumb suggests that ResNet has has worked in this case.\n",
    "\n",
    "`` The idea is to form a subblock with a 1x1 convolution reducing the number of features, a 3x3 convolution and another 1x1 convolution to restore the number of features to the original. The output of these convolutions is then added to the original tensor (bypass connection). I use 4 such subblocks by block, plus a single 1x1 convolution to increase the feature count after each pooling layer. ``\n",
    "\n",
    "\n",
    "\n",
    "### PyTorch model creation\n",
    "\n",
    "The branch model is composed of 6 blocks, each block processing maps with smaller and smaller resolution,, with intermediate pooling layers.\n",
    "\n",
    "    Block 1 - 384x384\n",
    "    Block 2 - 96x96\n",
    "    Block 3 - 48x48\n",
    "    Block 4 - 24x24\n",
    "    Block 5 - 12x12\n",
    "    Block 6 - 6x6\n",
    "\n",
    "> Block 1 has a single convolution layer with stride 2 followed by 2x2 max pooling. Because of the high resolution, it uses a lot of memory, so a minimum of work is done here to save memory for subsequent blocks.\n",
    "\n",
    "> Block 2 has two 3x3 convolutions similar to VGG. These convolutions are less memory intensive then the subsequent ResNet blocks, and are used to save memory. Note that after this, the tensor has dimension 96x96x64, the same volume as the initial 384x384x1 image, thus we can assume no significant information has been lost.\n",
    "\n",
    "> Blocks 3 to 6 perform ResNet like convolution. I suggest reading the original paper, but the idea is to form a subblock with a 1x1 convolution reducing the number of features, a 3x3 convolution and another 1x1 convolution to restore the number of features to the original. The output of these convolutions is then added to the original tensor (bypass connection). I use 4 such subblocks by block, plus a single 1x1 convolution to increase the feature count after each pooling layer.\n",
    "\n",
    "> The final step of the branch model is a global max pooling, which makes the model robust to fluke not being always well centered.\n",
    "\n",
    "\n",
    "I'm acknowledging this [notebook](https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563) because I incorporate few preprocessing techniques to make it work. When it comes to data preprocessing, I look what rule of thumb suggests because they make life so much easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other interesting methods:\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Generic One shot siamese network.\n",
    "- Learning a vector representation of a complex input, like an image, is an example of dimensionality reduction. \n",
    "- Taking <strong> Contrastive loss </strong> where Distance between two embeddings of similar class are optimized by bringing it closer, and .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder Network Architecture \n",
    "\n",
    "I've tried to keep everything succinct and detailed. \n",
    "\n",
    "An encoder CNN architecture. Instead of an MLP, which used linear, fully-connected layers, we can use:\n",
    "* [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), which can be thought of as stack of filtered images.\n",
    "* [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), which reduce the x-y size of an input, keeping only the most _active_ pixels from the previous layer.\n",
    "* The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.\n",
    "* Batch Normalization layer: The motivation behind it is purely statistical: it has been shown that normalized data, i.e., data with zero mean and unit variance, allows networks to converge much faster. So we want to normalize our mini-batch data, but, after applying a convolution, our data may not still have a zero mean and unit variance anymore. So we apply this batch normalization after each convolutional layer.\n",
    "\n",
    "### What about selecting the right kernel size?\n",
    "We always prefer to use smaller filters, like 3×3 or 5×5 or 7×7, but which ones of theses works the best? \n",
    "\n",
    "<br />\n",
    "\n",
    "\n",
    "#### Now it is seldom used in practice to create your own encoder network uniquely from scratch because there are so many  architecture  that will do the job. And these architectures are implemented in different frameworks.\n",
    "\n",
    "Example: \n",
    "\n",
    "ResNet18, VGG-16 etc. Slight modification to these networks will do the job. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(1, 96, kernel_size=11,stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(30976, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(128,2))\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # forward pass of input  as embedding\n",
    "        output1 = self.forward_once(input1)\n",
    "        return output1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note: This notebook only briefs about how I've build the model which is yet to be tested. Theoretically, this model should solve the problem optimally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to the solution:\n",
    "    \n",
    "To train our model, we need to first make it learn which class does a one belong to. \n",
    "       \n",
    "     - Initially we take an image (class/label Z), call it as anchor image. After encoding, we represent this image somewhere in Euclidean space with D dimensions, let's assume the location is A.\n",
    "     - We take the another image of same class Z, call it as positive. We represent this image somewhere in the same Euclidean space with D dimensions,  say B.\n",
    "     - A different third image is picked with different class, say Y, and call it as negative, represent in same space at point C. The below picture captures Anchor, positie and negative beautifully.\n",
    "\n",
    "Now our objective is to train the model such that <strong>same</strong> class images should be close that different class images. In short, let's consider $d$ as function of distance (generally, $L_2$ because it gives the squared value), then\n",
    "\n",
    "<center>$d(anchor, negative) > d(anchor, postive)$</center>\n",
    "and it should be at least by a margin. \n",
    "\n",
    " \n",
    "    \n",
    "![Obama](obama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Triplet loss?\n",
    "\n",
    "[Reference](https://www.youtube.com/watch?v=d2XB5-tuCWU)\n",
    "\n",
    "Rather than calculating loss based on two examples ( contrastive loss ), triplet loss involves an anchor example and one positive or matching example (same class) and one negative or non-matching example (differing class).\n",
    "\n",
    "The loss function penalizes the model such that the distance between the matching examples is reduced and the distance between the non-matching examples is increased. Explanation: \n",
    "For some distance on the embedding space d, the loss of a triplet (a,p,n) is:\n",
    "<center> $L=max(d(a,p)−d(a,n)+margin,0)$ </center>\n",
    "\n",
    "We minimize this loss, which pushes $d(a,p)$ to 0 and $d(a,n)$ to be greater than $d(a,p)+margin$. As soon as n becomes an “easy negative”, the loss becomes zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how do I build such a loss function optimally?\n",
    "\n",
    "#### Keyword: Optimally.\n",
    "\n",
    "Let me first acknowledge this [paper](https://arxiv.org/abs/1703.07737). \n",
    "\n",
    "            Images are now referred as embeddings.\n",
    "            \n",
    "Before calculating the loss,  we need sample only the relevant triplets (i.e anchor, positive and negative). To explain it much better, let's categorize the triplets in three different categories:\n",
    "- Easy negative:  The one where $d(negative, anchor) >> d(positive, anchor)$, if this is the case, $L$ will be zero (recall from previous cell). So implicitly, there wont be gradient that will be propagated backwards.\n",
    "- Hard negative: The case where $d(negative, anchor) < d(positive, anchor.)$. This means that network performed poorly, and there will be a significant gradient calculated to modify the weights (based on optimization).\n",
    "- Semi-hard: tiplets where the negative is not closer to the anchor than the positive, but which still have positive loss: $d(a,p)<d(a,n)<d(a,p)+margin$\n",
    "\n",
    "<br />\n",
    "\n",
    "##### Offline triplet mining\n",
    "\n",
    "- The first way to produce triplets is to find them offline, at the beginning of each epoch for instance. We compute all the embeddings on the training set, and then only select hard or semi-hard triplets. We can then train one epoch on these triplets.\n",
    "- Concretely, we would produce a list of triplets (i,j,k). We would then create batches of these triplets of size B, which means we will have to compute 3B embeddings to get the B triplets, compute the loss of these B triplets and then backpropagate into the network.\n",
    "- Overall this technique is not very efficient since we need to do a full pass on the training set to generate triplets. It also requires to update the offline mined triplets regularly.\n",
    "\n",
    "\n",
    "##### Online triplet mining\n",
    "\n",
    "The idea here is to compute useful triplets on the fly, for each batch of inputs. Given a batch of B examples (for instance B images of faces), we compute the B embeddings and we then can find a maximum of B3 triplets. Of course, most of these triplets are not valid (i.e. they don’t have 2 positives and 1 negative).\n",
    "\n",
    "Suppose that you have a batch of whale flukes as input of size B=PK, composed of P different flukes with K images each. A typical value is K=4\n",
    "    \n",
    ". The two strategies are:\n",
    "\n",
    "   batch all: \n",
    "    - select all the valid triplets, and average the loss on the hard and semi-hard triplets.\n",
    "    - a crucial point here is to not take into account the easy triplets (those with loss 0), as averaging on them would make the overall loss very small this produces a total of PK(K−1)(PK−K) triplets (PK anchors, K−1 possible positives per anchor, PK−K possible negatives)\n",
    "\n",
    "   batch hard: \n",
    "    - for each anchor, select the hardest positive (biggest distance d(a,p)) and the hardest negative among the batch this produces PK triplets \n",
    "    - the selected triplets are the hardest among the batch\n",
    "\n",
    "\n",
    "### Important Note:\n",
    "\n",
    "As a machine learning practitioner, it is believed in community that one way of implementing an algorithm on a dataset will not always yield a similar result for different dataset. So naturally, we should always explore different options. \n",
    "\n",
    "<br />\n",
    "\n",
    "Since, I prefer to write in PyTorch, there's an equivalent code for implementing batch-hard-strategy <strong>criterion</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## Courtesy: https://github.com/NegatioN/OnlineMiningTripletLoss/\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False, device='cpu'):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels, device).float()\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = mask_anchor_positive * pairwise_dist\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).float()\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    tl = hardest_positive_dist - hardest_negative_dist + margin\n",
    "    tl[tl < 0] = 0\n",
    "    triplet_loss = tl.mean()\n",
    "\n",
    "    return triplet_loss\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    anchor_positive_dist = pairwise_dist.unsqueeze(2)\n",
    "    anchor_negative_dist = pairwise_dist.unsqueeze(1)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    triplet_loss = mask.float() * triplet_loss\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss[triplet_loss < 0] = 0\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = triplet_loss[triplet_loss > 1e-16]\n",
    "    num_positive_triplets = valid_triplets.size(0)\n",
    "    num_valid_triplets = mask.sum()\n",
    "\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets.float() + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    dot_product = torch.matmul(embeddings, embeddings.t())\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = torch.diag(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances[distances < 0] = 0\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = distances.eq(0).float()\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = (1.0 -mask) * torch.sqrt(distances)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = torch.eye(labels.size(0)).byte()\n",
    "    indices_not_equal = ~indices_equal\n",
    "    i_not_equal_j = indices_not_equal.unsqueeze(2)\n",
    "    i_not_equal_k = indices_not_equal.unsqueeze(1)\n",
    "    j_not_equal_k = indices_not_equal.unsqueeze(0)\n",
    "\n",
    "    distinct_indices = (i_not_equal_j & i_not_equal_k) & j_not_equal_k\n",
    "\n",
    "\n",
    "    label_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    i_equal_j = label_equal.unsqueeze(2)\n",
    "    i_equal_k = label_equal.unsqueeze(1)\n",
    "\n",
    "    valid_labels = ~i_equal_k & i_equal_j\n",
    "\n",
    "    return valid_labels & distinct_indices\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels, device):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = torch.eye(labels.size(0)).byte().to(device)\n",
    "    indices_not_equal = ~indices_equal\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "\n",
    "    return labels_equal & indices_not_equal\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "\n",
    "    return ~(labels.unsqueeze(0) == labels.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
