{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(-1.0, requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    y = w*x + b\n",
    "    #y.backward()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  [[5.]]\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n",
      "[Parameter containing:\n",
      "tensor([[-0.0528]], requires_grad=True), Parameter containing:\n",
      "tensor([0.7261], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3.0]])\n",
    "y_pred = forward(x)\n",
    "\n",
    "# compute gradients\n",
    "y_pred.backward()\n",
    "\n",
    "\n",
    "y_pred = np.array(forward(x).detach())\n",
    "\n",
    "\n",
    "# Display gradients\n",
    "print('y_pred: ', y_pred)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)\n",
    "\n",
    "\n",
    "# we can create a random linear regression model using Linear function of pyTorch\n",
    "\n",
    "\n",
    "model_linear = Linear(in_features = 1, out_features = 1)\n",
    "\n",
    "print(list(model_linear.parameters()))\n",
    "\n",
    "x = torch.tensor([[1.0]])\n",
    "\n",
    "y_pred = np.array(model_linear(x).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.2150]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.8762], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(147.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(23.4939, grad_fn=<MeanBackward0>)\n",
      "tensor(3.7528, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5994, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        nn.Module.__init__(self)\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model_lr = LR(1,1)\n",
    "print(list(model_lr.parameters()))\n",
    "\n",
    "x = torch.tensor([[1.0],[2.0]])\n",
    "\n",
    "y_pred = np.array(model_lr(x).detach())\n",
    "\n",
    "\n",
    "\n",
    "w = torch.tensor(-10.0, requires_grad=True)\n",
    "X = torch.arange(-3,3,0.1).view(-1,1)\n",
    "\n",
    "f = -3*X\n",
    "\n",
    "plt.plot(X.numpy(), f.numpy())\n",
    "plt.plot(X.numpy(), f.numpy(),'g.')\n",
    "plt.show()\n",
    "\n",
    "def forward(x):\n",
    "    y = w*x\n",
    "    return y\n",
    "\n",
    "def loss_criterion(yhat, y):\n",
    "    return torch.mean((yhat - y)**2)\n",
    "\n",
    "alpha = 0.1\n",
    "y = f\n",
    "loss_arr = []\n",
    "for epoch in range(4):\n",
    "    y_pred = forward(X)\n",
    "    loss = loss_criterion(y_pred, y)\n",
    "    loss_arr.append(loss)    \n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    w.data = w.data - alpha*w.grad.data\n",
    "    w.grad.data.zero_()\n",
    "\n",
    "loss_arr = torch.tensor(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(316.6465, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 316.6465148925781\n",
      "tensor(25.8414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 25.841432571411133\n",
      "tensor(2.1212, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 2.1212351322174072\n",
      "tensor(0.1863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.18630355596542358\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.02832891419529915\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.015296826139092445\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.014088835567235947\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.013846933841705322\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.013685404323041439\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.013532000593841076\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.013380863703787327\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.013231384567916393\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.013083663769066334\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.012937527149915695\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.012793084606528282\n",
      "tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.012650245800614357\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.012508961372077465\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.012369275093078613\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.012231161817908287\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.012094571255147457\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.01195952296257019\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.011825992725789547\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.01169390045106411\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.011563361622393131\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.011434191837906837\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.01130651030689478\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.011180250905454159\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.01105540618300438\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.010931936092674732\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.010809854604303837\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.010689168237149715\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.010569802485406399\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.010451782494783401\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.010335049591958523\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.010219642892479897\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.010105514898896217\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.009992662817239761\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.009881078265607357\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.00977074820548296\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.009661633521318436\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.00955373328179121\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.009447065182030201\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.009341588243842125\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.009237262420356274\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.009134102612733841\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.009032105095684528\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.00893124844878912\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.008831514045596123\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.008732910268008709\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.008635368198156357\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.008538927882909775\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.00844358280301094\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.00834930595010519\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.008256080560386181\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.008163869380950928\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.008072677068412304\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.007982563227415085\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.00789340678602457\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.007805274799466133\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.00771811418235302\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.007631953340023756\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.007546696346253157\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.007462441921234131\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.00737908435985446\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.0072966995649039745\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.007215204648673534\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.007134623825550079\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.007054963614791632\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.006976175121963024\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.006898294668644667\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.006821243558079004\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.006745079532265663\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.006669757887721062\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.0065952870063483715\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.006521622650325298\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.006448814645409584\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.006376792676746845\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.006305586080998182\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.006235186941921711\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.006165543105453253\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.00609668018296361\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.006028622388839722\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.005961285438388586\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.005894729867577553\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.005828893277794123\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.005763815715909004\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.005699459929019213\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.00563581008464098\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.005572858732193708\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.00551063846796751\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.005449080839753151\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.005388248711824417\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.005328081082552671\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.005268569570034742\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.0052097514271736145\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.0051515898667275906\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.005094046238809824\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.005037172231823206\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.004980911500751972\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.004925277549773455\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LR(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-3,3,0.1).view(-1,1)\n",
    "        self.y = -3*X + 1\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = Data()\n",
    "\n",
    "def loss_criterion_py(yhat, y):\n",
    "        return nn.MSELoss()\n",
    "    \n",
    "trainloader = DataLoader(dataset, batch_size = 1)\n",
    "\n",
    "model = LR(1,1)\n",
    "param = model.parameters()\n",
    "\n",
    "optimizer = torch.optim.SGD(param, lr = 0.01)\n",
    "\n",
    "for epoch in range(0):\n",
    "    for x,y in trainloader:\n",
    "        y_pred_pt = model(x)\n",
    "        loss = loss_criterion_py(y_pred_pt, y)\n",
    "        optimizer.zero_grad()\n",
    "        #differentiate the loss\n",
    "        loss.backward()\n",
    "        #update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
