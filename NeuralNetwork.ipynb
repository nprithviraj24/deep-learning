{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   (1, 64)\n",
      "Weights:  [(0,), (64, 24), (24, 16), (16, 10)]\n",
      "act shapes:  [(64, 1), (24, 1), (16, 1), (10, 1)]\n",
      "logits shapes:  [(0,), (24, 1), (16, 1), (10, 1)]\n",
      "delta: at 3  (1, 10)\n",
      " logits at softmax (16, 1)\n",
      "new delta: at 2  (1, 16)\n",
      "-------------------------\n",
      "weights:  (24, 16)  delta:  (1, 16)\n",
      "bsd:  (24, 1)\n",
      " logit to reul shape:  (24, 1)\n",
      "new delta:  (24, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RELU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_der(z):\n",
    "    return 1.*(z>0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_der(x):\n",
    "    prob = softmax(x)\n",
    "#     print(np.shape(prob))\n",
    "    r = np.matmul(prob.T, (np.ones_like(prob)-prob))\n",
    "#     print(np.shape(r))\n",
    "    return r\n",
    "\n",
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual    \n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    l = np.log(ypred)\n",
    "    return -np.sum(l*yactual)\n",
    "#     logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "#     xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "#     return xentropy\n",
    "    \n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "\n",
    "\n",
    "\n",
    "class neuralNetwork():    \n",
    "    FCreluLayers = 0  \n",
    "    Weights = []\n",
    "    Biases = []\n",
    "    softmax = False\n",
    "    softmaxClasses = 0\n",
    "    loss = []\n",
    "    delta = [[]];    dW = [[]];    db = [[]]\n",
    "    act = []\n",
    "    logits = [[]]\n",
    "    def FullyConnectedreluLayers(self, values):\n",
    "        assert len(values) == self.FCreluLayers+1, \"Number of fully connected layers are not the same.\"\n",
    "        self.Weights.append([])\n",
    "        self.Biases.append([])\n",
    "        for i in range(1, self.FCreluLayers+1):            \n",
    "            self.Weights.append(np.random.normal(0, 0.1, ([values[i-1], values[i]])))\n",
    "            self.Biases.append(np.random.normal(0, 0.1, ([values[i], 1])))\n",
    "            self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "               \n",
    "    \n",
    "    def softmaxLayer(self):\n",
    "        previous = self.Weights[-1].shape[1]\n",
    "        self.Weights.append(np.random.normal(0,0.1,[previous, self.softmaxClasses]))\n",
    "        self.Biases.append(np.random.normal(0, 0.1, (self.softmaxClasses, 1)))\n",
    "        self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "        \n",
    "    def calcLoss(ypred, yactual):\n",
    "        if self.softmax == True:\n",
    "            return crossEntropy(ypred,yactual, self.softmaxClasses)\n",
    "    \n",
    "    def feedForward(self,X):\n",
    "#         a = []\n",
    "        self.act.append(X.T)\n",
    "#         l = self.FCLayers if softmax == False else self.FCLayers-1\n",
    "        \n",
    "        for i in range(1, self.FCreluLayers+1):\n",
    "#             print(\" weights shape: \", np.shape(self.Weights[i].T) , \"a shape: \", np.shape(self.act[i-1]))\n",
    "            self.logits.append(np.matmul(self.Weights[i].T, self.act[i-1]) + self.Biases[i])\n",
    "            self.act.append( RELU(self.logits[-1]) )\n",
    "            \n",
    "#         print(\"-----\")\n",
    "        if self.softmax == True:\n",
    "            self.logits.append(np.matmul(self.Weights[-1].T, self.act[-1]) + self.Biases[-1])\n",
    "            self.act.append(softmax(self.logits[-1]))\n",
    "#             print(np.sum(self.act[-1]))   \n",
    "#             print(self.act[-1])\n",
    "        print(\"Weights: \", [np.shape(self.Weights[i]) for i in range(len(self.Weights))])\n",
    "        print(\"act shapes: \", [np.shape(self.act[i]) for i in range(len(self.act))])\n",
    "        print(\"logits shapes: \", [np.shape(self.logits[i]) for i in range(len(self.logits))])\n",
    "        return self.act[-1]\n",
    "    \n",
    "    def backprop(self, ypred, yact):\n",
    "        L = len(self.delta)-1       \n",
    "        self.delta[L] = delta_cross_entropy(ypred,yact)\n",
    "        print(\"delta: at 3 \", np.shape(self.delta[L]))\n",
    "#         print(self.delta[L])\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L]\n",
    "#         print(self.dW[L])\n",
    "#         print(\"1 act shape: \", np.shape(self.act[L-1]), \" delt shape: \", np.shape(self.delta[L]), \"  dW shape: \", np.shape(self.dW[L]))\n",
    "        \n",
    "#         print(\"dw shape: \", np.shape(self.dW[L]))\n",
    "        asd = self.Weights[L] * self.delta[L].T\n",
    "        print(\" logits at softmax\", np.shape(self.logits[L-1]))\n",
    "#         print(asd)\n",
    "#         print(\"asd: \", np.shape(asd))\n",
    "        self.delta[L-1] = np.multiply(softmax_der(self.logits[L-1]), asd.T)\n",
    "        print(\"new delta: at 2 \", np.shape(self.delta[L-1]))\n",
    "#         print(self.delta[L-1])\n",
    "\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "        L = 2\n",
    "#         self.delta[L] = delta_cross_entropy(ypred,yact)\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L]\n",
    "#         print(\"2 act shape: \", np.shape(self.act[L-1]), \" delt shape: \", np.shape(self.delta[L]), \"  dW shape: \", np.shape(self.dW[L]))\n",
    "#         print(\"dw shape: \", np.shape(self.dW[L]))\n",
    "        bsd = self.Weights[L]* self.delta[L].T\n",
    "        print(\"weights: \", np.shape(self.Weights[L]), \" delta: \", np.shape(self.delta[L]))\n",
    "#         print(\"logit previous layer: \", np.shape())\n",
    "        reluOut = relu_der(self.logits[L-1])\n",
    "        print(\"bsd: \", np.shape(bsd))\n",
    "        print(\" logit to reul shape: \", np.shape(self.logits[L-1]))\n",
    "        self.delta[L-1] = np.multiply(reluOut, bsd)\n",
    "        print(\"new delta: \", np.shape(self.delta[L-1]))\n",
    "        \n",
    "        \n",
    "        L = 1        \n",
    "#         self.delta[L] = delta_cross_entropy(ypred,yact)\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "#         print(\"act shape: \", np.shape(self.act[L-1]), end=\" \")\n",
    "#         print(\" delta shape: \", np.shape(self.delta[L]), end=\"\")\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L].T\n",
    "#         print(\"act shape: \", np.shape(self.act[L-1]), \" delt shape: \", np.shape(self.delta[L]), \"  dW shape: \", np.shape(self.dW[L]))\n",
    "#         L -= 1\n",
    "# #         print(\"shape delta:\", delta[l].shape, \" a shape: \", a[l-1].shape, \"  dW shape: \", dW[l].shape)\n",
    "#         if l > 1:\n",
    "#             print(\"W[l] shape: \", W[l].shape)\n",
    "#             delta[l-1] = np.multiply(actder(z[l-1]), W[l] * delta[l])\n",
    "#         return a[-1]\n",
    "#         return [np.shape(self.Weights[i]) for i in range(len(self.Weights))]\n",
    "        alpha = 0.1\n",
    "        for l in range(1,len(self.Weights)):            \n",
    "            self.Weights[l] = self.Weights[l] - alpha * self.dW[l]\n",
    "#             print(type(self.Weights[l]))\n",
    "            self.Biases[l] = self.Biases[l] - alpha * self.db[l]\n",
    "\n",
    "\n",
    "mnist = neuralNetwork()\n",
    "mnist.__class__.FCreluLayers = 2 #exclusing the softmax\n",
    "mnist.__class__.softmax = True\n",
    "mnist.__class__.softmaxClasses = 10\n",
    "N = 64\n",
    "h1 = 24\n",
    "h2 = 16\n",
    "mnist.FullyConnectedreluLayers([N,h1,h2])\n",
    "mnist.softmaxLayer()\n",
    "print(\"input:  \", np.shape(np.matrix(np.ones(64))))\n",
    "ypred = mnist.feedForward( np.matrix(np.ones(64)) )\n",
    "# print(np.shape(ypred), np.zeros(10))\n",
    "mnist.backprop(ypred, np.matrix(np.zeros(10)) )\n",
    "# mnist.loss.append(mnist.calcLoss(ypred, yt))\n",
    "# print(mnist.loss[-1])\n",
    "\n",
    "# print([np.shape(mnist.Weights[i]) for i in range(len(mnist.Weights))])  \n",
    "# print(mnist.feedForward(np.matrix([0,1,2,3,4,5]), 3 ))\n",
    "# mnist.layerWeights[-1].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_digits\n",
    "# digits = load_digits()\n",
    "# print(\"listing digits: \", list(digits))\n",
    "# print(\"Number of samples: \", digits.data.shape[0])\n",
    "# print(\"Size of each sample: \", digits.data.shape[1])\n",
    "# print(\"Listing target names: \", list(digits.target_names))\n",
    "\n",
    "# y = np.matrix(digits.target).T\n",
    "# X = np.matrix(digits.data)\n",
    "# print(\"Image at index 3: \")\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.matshow(digits.images[3]) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# r = np.array([1,0,0])\n",
    "# logits = np.array([0.6,0,0.5])\n",
    "# logits_for_answers = logits[np.arange(len(logits)),r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.matrix(np.zeros(5)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "[0.47000363 1.22377543 0.33647224 1.38629436]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.08411805915530322"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual\n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    print(yactual)\n",
    "    l = np.log(ypred)\n",
    "    print(l)\n",
    "    return - np.sum(l*yactual)/yactual.shape[0]\n",
    "\n",
    "crossEntropy(np.array([1.6,3.4,1.4,4]), 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.08279633e-05,  8.37990924e-05,  2.49801574e-01, -2.49916201e-01])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "delta_cross_entropy(np.array([4,3,2,1]), np.array([3,4,12,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_der(np.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
