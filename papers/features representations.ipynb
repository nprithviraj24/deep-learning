{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "visualise the data, <br />\n",
    "preprocess it, <br />\n",
    "define the model, <br />\n",
    "define loss and optimization function, <br />\n",
    "train the model, <br />\n",
    "stop when validation and training loss at min, <br />\n",
    "test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  CNNs shine because they are localise each region and try to find the object.\n",
    "<br />\n",
    "MNIST works in MLP and CNN because data is preprocessed and each number is at center and are of equal size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP vs CNN\n",
    "\n",
    "MLP uses lot of parameters <br />\n",
    "MLP throws away 2D info. So spatial information or knowledge of where the pixels are located in reference to each other is relavent to understand the images and could aid significantly elucidating the patterns contained in the pixel values.\n",
    "\n",
    "CNN uses sparsely connected layers\n",
    "Accepts matrices as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency in images\n",
    "\n",
    "\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, frequency in images is a rate of change. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "High-frequency components also correspond to the edges of objects in images, which can help us classify those objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters!!\n",
    "\n",
    "- To filter out unwanted information\n",
    "- Amplify features of interests like edges or boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Pass filters:\n",
    "\n",
    "- Used to sharpen an image\n",
    "- Enhance high-frequency parts of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "- Edges are areas in images where intesity changes quickly, and these edges often indicate object boundaries.\n",
    "\n",
    "But how?\n",
    "\n",
    "#### convolutional kernel\n",
    "- Kernals are just grid of number that modify an image.\n",
    "- Kernals must be odd squared matrix because elements in middle of each row/column must hold an interesting property. Check Sober filters. <br />\n",
    "Example of Kernal: Edge detection kernal whose elements all sum to zero. Its important that it should be all summed to zero as it is computing the difference or change between the neighbouring pixels.\n",
    "\n",
    "Center pixel is the most important, followed by close pixels on top and botton, and left and right. These adjacent values are negative that increase the contrast in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Handling\n",
    "\n",
    "<strong>Kernel convolution</strong> relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "<strong>Extend</strong> The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "<strong>Padding</strong> The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "<strong>Crop</strong> Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhancing horizontal edges and lines in an image by using this filter:   <strong>Bottom Sobel</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -2 -1]\n",
      " [ 0  0  0]\n",
      " [ 1  2  1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "a = numpy.asarray([[-1,-2,-1], [0,0,0], [1,2,1]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Sobel filter \n",
    "is very commonly used in edge detection and in finding patterns in intensity in an image. Applying a Sobel filter to an image is a way of taking (an approximation) of the derivative of the image in the x or y direction, separately. The operators look as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer : Stack of feature maps\n",
    "is where you extract edges from an image. The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image. <br />\n",
    "\n",
    "\n",
    "So 4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4! In practice each of these images generated from 4 different filters are called <strong>feature maps</strong> or <strong>activation maps</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning\n",
    "\n",
    "In the code you've been working with, you've been setting the values of filter weights explicitly, but neural networks will actually learn the best filter weights as they train on a set of image data. We know that high-pass and low-pass filters are what define the behavior of a network.\n",
    "\n",
    "In practice, you'll also find that many neural networks learn to detect the edges of images because the edges of object contain valuable information about the shape of an object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN\n",
    "detects what kind of pattern it needs to detect based on the loss function.\n",
    "\n",
    "\n",
    "#### CNN learns filters!!!!\n",
    "We wont specify the values of filters or tell the CNN what kind of pattern it needs to detect. These will be learned from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of CNN!!\n",
    "\n",
    "- Number of filters: That tells the width of the Conv block.\n",
    "- Size of the filter: Alters the size of the detected pattern.\n",
    "- <strong>Stride</strong>: The amount by which filter slides through the image.\n",
    "    - Stride 1 means the generated feature map will almost (depends on the what we are doing to edge of the image) be as big as the original image.\n",
    "    - As we increase the size, the size of the generated feature map decreases.\n",
    "- Padding: Makes sure that edge of the images also contribute to the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling : Intuition\n",
    "\n",
    "A complicated dataset with many different object categories will require large number of filters, each responsible for finding pattern in the image.  More filters means bigger stack, which means the dimensionality  of ouw convolutional layers can get quite large. \n",
    "\n",
    "Higher dimensionality means we will need to use more parameters, which can lead to overfitting, this we need a method for reducing this dimensionality. This is the role of pooling layers within a CNN.\n",
    "\n",
    "Two different types of pooling: \n",
    "- Average\n",
    "- Max\n",
    "\n",
    "These pooling layers have their own \"kernel\" type functionality. A pooling layer considers a window of size, say 2x2 and with stride 2 will divide the stack of features maps into half, i.e new width and height of features maps will equal to half of previous convolutional layer. \n",
    "\n",
    "- Max pooling layers takes the maximum values out of 4 values. (if 2x2)\n",
    "- Average takes the average value of the same 4 values. (if 2x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Padding is just adding a border of pixels around an image. In PyTorch, you specify the size of this border.\n",
    "\n",
    "### Why do we need padding?\n",
    "\n",
    "When we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n",
    "\n",
    "The most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value. You can read more about calculating the amount of padding, given a kernel_size, here.\n",
    "\n",
    "\n",
    "If you overlay a 7x7 kernel so that its center-pixel is at the right-edge of an image, you will have 3 kernel columns that do not overlay anything! So, that's how big your padding needs to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers\n",
    "\n",
    "We typically define a convolutional layer in PyTorch using nn.Conv2d, with the following parameters, specified:\n",
    "\n",
    "```nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)```\n",
    "\n",
    "   - ```in_channels``` refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "   - ```out_channels``` refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "   - ```kernel_size``` is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "   - ```stride``` and ```padding``` have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation:\n",
    "\n",
    "The job of convolutional neural network is to discover patterns contained in an image. <br />\n",
    "A sequence of layers in responsible for this discovery. <br />\n",
    "The layers in a CNN convert an input image array into a representation that encodes only the content of the image. This is often called feature level representation of an image or a feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
