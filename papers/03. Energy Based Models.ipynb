{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Energy Based Models (EBM)\n",
    "\n",
    "By Yann Lecun (ICLR 2020, and [Original Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)\n",
    "\n",
    "\n",
    "#### Problem it addresses:\n",
    "##### Problem of Integrals\n",
    "\n",
    "Much of deep learning literature uses probability models\n",
    "as loss functions but the main problem with probability based models is that they\n",
    "must be properly normalised, which sometimes require evaluating intractable integrals\n",
    "over space of all the variables configurations.\n",
    "\n",
    "<strong>EBM</strong> has only loss function that gives a \"happy\" value.\n",
    "If it is happy, it will give less value. If it is not happy,\n",
    "ithen it will blow up.\n",
    "\n",
    "##### So how is it any different from the probability based models?\n",
    "\n",
    "EBM can be anything if it fits the earlier definition. An Inverse Probability Measure\n",
    "is also an EBM.\n",
    "\n",
    "<strong> Every probability measure is a EBM, and every EBM can be changed to probability Measure if it is normalised.</strong>\n",
    "Normalized over Gibbs distribution. (Probabilistic Graphical Models)\n",
    "\n",
    "Often time normalising can be a tedious or impossible task as we have to deal with\n",
    "<strong>intractable integrals</strong>. The idea is that we have to check the probability of one\n",
    "instance with every other instance that could exist which is not possible practically.\n",
    "\n",
    "##### Following section expects that reader is well aware of Manifold Hypothesis.\n",
    "\n",
    "<strong> Independent explanatory factors of variation </strong>\n",
    "\n",
    "Every input (image/sequence of words) that has an semantic meaning has a property\n",
    " that it lies somewhere in the high dimensional space. Such inputs are related to each other\n",
    "  lie on a manifold in that space, and if we traverse through it will yield a arbitrary variation in each of these\n",
    "   inputs. We achieve it through changing values of Latent Variables that represents\n",
    "   independent explanatory factors of variation.\n",
    "\n",
    " <strong> What is Energy in this context? </strong>\n",
    "\n",
    "If a well-trained encoder projects a face image (say) on or near a face manifold (in high dimensional space) then we say that\n",
    " the associated energy of that model is low. But if the encoder projects is outside the manifold then high energy. This forces\n",
    " the neural network to have less tolerance towards unexpected and unseen output.\n",
    "\n",
    " #### Inference\n",
    "\n",
    " For a given input $x$ and output label $y$, find the values of $y$ that makes\n",
    "  $F(x,y)$ small.\n",
    "  <center>\n",
    "   $ y' = \\argmin F(x,y)$\n",
    "   </center>\n",
    "\n",
    "   <break>\n",
    "   If $y$ is continous, apply gradient descend to find optimal $y$.\n",
    "\n",
    "   If $y$ is discrete, how?\n",
    "\n",
    "   #### Exploring EBMs\n",
    "\n",
    "   Machine Learning is learning about the distribution of data.\n",
    "\n",
    "  <strong> K-means vs GMM </strong>\n",
    "\n",
    "  We create a 'happy' function that is happy with any point in the\n",
    "   data distribution. In K-means we associated with closest cluster center, hence\n",
    "   we are not normalizing anything, so it is a energy based methods and not a probability\n",
    "   based method.\n",
    "\n",
    "  However, in GMMs we have to consult with clusters (or Gaussians) and know about our datapoint.\n",
    "   We have to find the membership of this point with every other Gaussian. Hence, there inherently exists\n",
    "   a normalization term in the cost function of GMM.\n",
    "\n",
    "   <center>\n",
    "   $F(x,y) = - \\frac{1}{B} log \\int e$ <sup>- $ \\beta E(x,y,z)$</sup>  $\n",
    "   </center>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}