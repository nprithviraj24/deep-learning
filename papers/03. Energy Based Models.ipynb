{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Energy Based Models (EBM)\n",
    "\n",
    "By Yann Lecun (ICLR 2020, and [Original Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)\n",
    "\n",
    "#### Following section expects that reader is well aware of Manifold Hypothesis.\n",
    "##### Problem of Integrals\n",
    "\n",
    "Much of deep learning literature uses probability models\n",
    "as loss functions but the main problem with probability based models is that they\n",
    "must be properly normalised, which sometimes require evaluating intractable integrals\n",
    "over space of all the variables configurations.\n",
    "\n",
    "<strong>EBM</strong> has only loss function that gives a \"happy\" value.\n",
    "If it is happy, it will give a less value. If it is not happy,\n",
    "then it will blow up.\n",
    "\n",
    "##### So how is it any different from the probability based models?\n",
    "\n",
    "EBM can be anything if it loosely fits the earlier definition. An Inverse Probability Measure\n",
    "is also an EBM.\n",
    "\n",
    "<strong> Every probability measure is a EBM, and every EBM can be changed to probability Measure if it is normalised.</strong>\n",
    "Normalized over Gibbs distribution. (Probabilistic Graphical Models)\n",
    "\n",
    "Often time normalising can be a tedious or impossible task as we have to deal with\n",
    "<strong>intractable integrals</strong>. The idea is that we have to check the probability of one\n",
    "instance with every other instance that could exist, which is not possible practically.\n",
    "\n",
    "<strong> Independent explanatory factors of variation </strong>\n",
    "\n",
    "Every input (image/sequence of words) with a semantic meaning has a property\n",
    " that its encoded form lies somewhere in the high dimensional space. Inputs that are semantically related\n",
    "  lie on a manifold in that space, and traversing through the manifold will yield an arbitrary variation in each of these\n",
    "   inputs. We achieve it through changing values of Latent Variables that represents\n",
    "   independent explanatory factors of variation.\n",
    "\n",
    " <strong> What is Energy in this context? </strong>\n",
    "\n",
    "If a well-trained encoder projects a face image (say) on or near a face manifold (in high dimensional space) then we say that\n",
    " the associated energy of that model is low. But if the encoder projects is outside the manifold then high energy. This forces\n",
    " the neural network to have less tolerance towards unexpected and unseen output.\n",
    "\n",
    " #### Inference\n",
    "\n",
    " For a given input $x$ and output label $y$, find the values of $y$ that makes\n",
    "  $F(x,y)$ small.\n",
    "   $ y' = \\argmin F(x,y)$\n",
    "\n",
    "   <break>\n",
    "   If $y$ is continous, apply gradient descend to find optimal $y$.\n",
    "\n",
    "   If $y$ is discrete, how?\n",
    "\n",
    "   #### Exploring EBMs\n",
    "\n",
    "   Machine Learning is learning about the distribution of data.\n",
    "\n",
    "  <strong> K-means vs GMM </strong>\n",
    "\n",
    "  We create a 'happy' function that is happy with any point in the\n",
    "   data distribution. In K-means we associated with closest cluster center, hence\n",
    "   we are not normalizing anything, so it is a energy based methods and not a probability\n",
    "   based method.\n",
    "\n",
    "  However, in GMMs we have to consult with clusters (or Gaussians) and know about our datapoint.\n",
    "   We have to find the membership of this point with every other Gaussian. Hence, there inherently exists\n",
    "   a normalization term in the cost function of GMM.\n",
    "\n",
    "\n",
    "   $F(x,y) = - \\frac{1}{B} log \\int e$ <sup>- $ \\beta E(x,y,z)$</sup>  $\n",
    "\n",
    "\n",
    "#### Recent surge of energy based models\n",
    "\n",
    "SimCLR and other contrastive based approaches leverage energy functions as their\n",
    "loss functions. Such models train to take pairs and create a manifold by forcing the NN\n",
    "to yield less energy for positive-positive pair, snf high energy for positive-negative pair.\n",
    "\n",
    "In a way, we can never really describe the manifold but what we can do is to build\n",
    "energy functions that\n",
    "basically tells us how far we are from the manifold.\n",
    "\n",
    "\n",
    "##### Language Model: BERT\n",
    "\n",
    "1. Take an input.\n",
    "2. Corrupt it.\n",
    "3. Build a system to distinguish between clean vs corrupted version.\n",
    "\n",
    "The idea is that we create a noisy/corrupted form of semantic sequences, so that the corrupted version\n",
    "can be turned to the actual one. We do this by encoded the semantic sequence somewhere off the manifold,\n",
    "and then model learn to drag these semantic sequences back to the manifold.\n",
    "\n",
    "Datapoints (encoded version of semantic sequences) that are all off the\n",
    "manifold are just noised version of ones that are in on the manifold.\n",
    "\n",
    "<strong> So how far should we push off from the manifold? If we push too far,\n",
    "will that at some point be too problematic? </strong>\n",
    "\n",
    "We might need smooth energy function, not a form of a canyon. If the depressions are small, and if the problem space is too sparse\n",
    " we can never really learn anything about the manifold.\n",
    "\n",
    " ##### Problem with Maximum Likelihood Estimation\n",
    "\n",
    "It wants to ake the difference between the energy on the data manifold, and the nergy just outside of it infinitely large.\n",
    "It wants to make the data manifold an infinitely deep and infinitely narrow canyons.\n",
    "\n",
    "\n",
    "Therefor, the loss must be regularized to keeo the energy smooth. Eg: Wasserstein GAN. This is\n",
    "dont so that gradient based inference works.\n",
    "\n",
    "If we don't regularize properly, we end up like dirac distribution everywhere and nothing in between. To throw away probability framework\n",
    " we lose the ability to make numerical predictions about how likely datapoints\n",
    " is you simply compare it to others,\n",
    "\n",
    " - Probabilistic models will always put a point into global space and compare.\n",
    " - Energy function is a scalar valued function that just gives us a number.\n",
    "\n",
    "\n",
    " Some algorithms are about representation learning.\n",
    "\n",
    " Some algorithms are about manifold learning.\n",
    "\n",
    " Here we are learning to rectify the corrupt input data. We can either modify our manifold, by twisting it\n",
    " and cuttin it, or we can learn the manifold in the data and ensure our network\n",
    "  projects to one.\n",
    "\n",
    "  ##### What about pertubations in visual domain?\n",
    "\n",
    "  One possible reason could be learned feature invariances makes it hard to pertubate\n",
    "  the input iamges. Maybe we don't have good ways to throw faces off the manifold."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}