{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RELU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_der(z):\n",
    "    return 1.*(z>0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_der(x):\n",
    "    prob = softmax(x)\n",
    "#     print(np.shape(prob))\n",
    "    r = np.matmul(prob.T, (np.ones_like(prob)-prob))\n",
    "#     print(np.shape(r))\n",
    "    return r\n",
    "\n",
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual    \n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    l = np.log(ypred)\n",
    "    return -np.sum(l.T*yactual)\n",
    "\n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y.T*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "\n",
    "\n",
    "\n",
    "class neuralNetwork():    \n",
    "    FCreluLayers = 0  \n",
    "    Weights = []\n",
    "    Biases = []\n",
    "    softmax = False\n",
    "    softmaxClasses = 0\n",
    "    loss = []\n",
    "    delta = [[]];    dW = [[]];    db = [[]]\n",
    "    act = []\n",
    "    logits = [[]]\n",
    "    def FullyConnectedreluLayers(self, values):\n",
    "        assert len(values) == self.FCreluLayers+1, \"Number of fully connected layers are not the same.\"\n",
    "        self.Weights.append([])\n",
    "        self.Biases.append([])\n",
    "        for i in range(1, self.FCreluLayers+1):            \n",
    "            self.Weights.append(np.random.normal(0, 0.1, ([values[i-1], values[i]])))\n",
    "            self.Biases.append(np.random.normal(0, 0.1, ([values[i], 1])))\n",
    "            self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "               \n",
    "    \n",
    "    def softmaxLayer(self):\n",
    "        previous = self.Weights[-1].shape[1]\n",
    "        self.Weights.append(np.random.normal(0,0.1,[previous, self.softmaxClasses]))\n",
    "        self.Biases.append(np.random.normal(0, 0.1, (self.softmaxClasses, 1)))\n",
    "        self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "        \n",
    "    def calcLoss(self, ypred, yactual):\n",
    "        if self.softmax == True:\n",
    "            return crossEntropy(ypred,yactual, self.softmaxClasses)\n",
    "    \n",
    "    def feedForward(self,X):\n",
    "#         self.act = [X.T]\n",
    "#         self.logits = [[]]\n",
    "        self.act.append(X.T)\n",
    "        for i in range(1, self.FCreluLayers+1):\n",
    "#             print(\"weights shape: \", np.shape(self.Weights[i].T), \" act: \", np.shape(self.act[i-1]))\n",
    "            self.logits.append(np.matmul(self.Weights[i].T, self.act[i-1]) + self.Biases[i])\n",
    "            self.act.append( RELU(self.logits[-1]) )\n",
    "\n",
    "        if self.softmax == True:\n",
    "            self.logits.append(np.matmul(self.Weights[-1].T, self.act[-1]) + self.Biases[-1])\n",
    "            self.act.append(softmax(self.logits[-1]))\n",
    "\n",
    "#         print(\"Weights: \", [np.shape(self.Weights[i]) for i in range(len(self.Weights))])\n",
    "#         print(\"act shapes: \", [np.shape(self.act[i]) for i in range(len(self.act))])\n",
    "#         print(\"logits shapes: \", [np.shape(self.logits[i]) for i in range(len(self.logits))])\n",
    "        return self.act[-1]\n",
    "    \n",
    "    def backprop(self, ypred, yact):\n",
    "        L = len(self.delta)-1\n",
    "        yact = oneHotEncode(yact, self.softmaxClasses)\n",
    "        \n",
    "#         self.delta=[[]]; self.\n",
    "        \n",
    "#         print(yact)\n",
    "        self.delta[L] = delta_cross_entropy(ypred,yact)\n",
    "#         print(\"delta_cross_entropy: \", np.sum(self.delta[L]))\n",
    "#         self.delta[L] = np.subtract(yact, ypred)\n",
    "#         print(np.shape(self.delta[L]))\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L].T\n",
    "\n",
    "        WxD = self.Weights[L] * self.delta[L]\n",
    "        self.delta[L-1] = np.multiply(softmax_der(self.logits[L-1]), WxD.T)\n",
    "        \n",
    "        L = 2\n",
    "\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L]\n",
    "        bsd = self.Weights[L]* self.delta[L].T\n",
    "#         print(\"weights: \", np.shape(self.Weights[L]), \" delta: \", np.shape(self.delta[L]))\n",
    "        reluOut = relu_der(self.logits[L-1])\n",
    "#         print(\"bsd: \", np.shape(bsd))\n",
    "#         print(\" logit to reul shape: \", np.shape(self.logits[L-1]))\n",
    "        self.delta[L-1] = np.multiply(reluOut, bsd)\n",
    "#         print(\"new delta: \", np.shape(self.delta[L-1]))\n",
    "        \n",
    "        \n",
    "        L = 1        \n",
    "\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L].T\n",
    "            \n",
    "        alpha = 0.01\n",
    "        for l in range(1,len(self.Weights)):            \n",
    "            self.Weights[l] = self.Weights[l] - alpha * self.dW[l]\n",
    "            self.Biases[l] = self.Biases[l] - alpha * self.db[l]\n",
    "# mnist.loss.append(mnist.calcLoss(ypred, yt))\n",
    "# print(mnist.loss[-1])\n",
    "\n",
    "# print([np.shape(mnist.Weights[i]) for i in range(len(mnist.Weights))])  \n",
    "# print(mnist.feedForward(np.matrix([0,1,2,3,4,5]), 3 ))\n",
    "# mnist.layerWeights[-1].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing digits:  ['data', 'target', 'target_names', 'images', 'DESCR']\n",
      "Number of samples:  1797\n",
      "Size of each sample:  64\n",
      "Listing target names:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "number of sample:  1797  each sample:  64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(\"listing digits: \", list(digits))\n",
    "print(\"Number of samples: \", digits.data.shape[0])\n",
    "print(\"Size of each sample: \", digits.data.shape[1])\n",
    "print(\"Listing target names: \", list(digits.target_names))\n",
    "\n",
    "y = np.matrix(digits.target).T\n",
    "X = np.matrix(digits.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "print(\"number of sample: \",M, \" each sample: \",N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):    \n",
    "    return X/255\n",
    "\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at random index 3: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Image at random index 3: \")\n",
    "import matplotlib.pyplot as plt \n",
    "plt.matshow(digits.images[3]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 145890.475149\n",
      "Iteration 1 loss 145997.420485\n",
      "Iteration 2 loss 146101.449739\n",
      "Iteration 3 loss 146401.994328\n",
      "Iteration 4 loss 147041.121692\n",
      "Iteration 5 loss 148376.212906\n",
      "Iteration 6 loss 150912.722555\n",
      "Iteration 7 loss 155145.804627\n",
      "Iteration 8 loss 161884.508707\n",
      "Iteration 9 loss 171647.419891\n",
      "Iteration 10 loss 184973.002671\n",
      "Iteration 11 loss 202140.076777\n",
      "Iteration 12 loss 223316.563503\n",
      "Iteration 13 loss 248621.113158\n",
      "Iteration 14 loss 278274.746451\n",
      "Iteration 15 loss 312627.848646\n",
      "Iteration 16 loss 352039.833160\n",
      "Iteration 17 loss 396372.193448\n",
      "Iteration 18 loss 445617.578241\n",
      "Iteration 19 loss 500568.818011\n",
      "Iteration 20 loss 561571.730418\n",
      "Iteration 21 loss 628955.237394\n",
      "Iteration 22 loss 703086.199767\n",
      "Iteration 23 loss 784039.576343\n",
      "Iteration 24 loss 872368.470020\n",
      "Iteration 25 loss 968040.450225\n",
      "Iteration 26 loss 1071400.538699\n",
      "Iteration 27 loss 1182752.268163\n",
      "Iteration 28 loss 1302375.187747\n",
      "Iteration 29 loss 1430072.473772\n",
      "Iteration 30 loss 1565274.643870\n",
      "Iteration 31 loss 1708993.249034\n",
      "Iteration 32 loss 1861578.885273\n",
      "Iteration 33 loss 2023497.687845\n",
      "Iteration 34 loss 2194724.898641\n",
      "Iteration 35 loss 2374571.283825\n",
      "Iteration 36 loss 2563592.276310\n",
      "Iteration 37 loss 2761245.631756\n",
      "Iteration 38 loss 2968994.185228\n",
      "Iteration 39 loss 3187298.391216\n",
      "Iteration 40 loss 3416124.946879\n",
      "Iteration 41 loss 3655710.552426\n",
      "Iteration 42 loss 3906595.471431\n",
      "Iteration 43 loss 4169042.697200\n",
      "Iteration 44 loss 4443099.786275\n",
      "Iteration 45 loss 4729015.305417\n",
      "Iteration 46 loss 5027125.340836\n",
      "Iteration 47 loss 5337674.694951\n",
      "Iteration 48 loss 5660975.206325\n",
      "Iteration 49 loss 5996816.402174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50 loss nan\n",
      "Iteration 51 loss nan\n",
      "Iteration 52 loss nan\n",
      "Iteration 53 loss nan\n",
      "Iteration 54 loss nan\n",
      "Iteration 55 loss nan\n",
      "Iteration 56 loss nan\n",
      "Iteration 57 loss nan\n",
      "Iteration 58 loss nan\n",
      "Iteration 59 loss nan\n",
      "Iteration 60 loss nan\n",
      "Iteration 61 loss nan\n",
      "Iteration 62 loss nan\n",
      "Iteration 63 loss nan\n",
      "Iteration 64 loss nan\n",
      "Iteration 65 loss nan\n",
      "Iteration 66 loss nan\n",
      "Iteration 67 loss nan\n",
      "Iteration 68 loss nan\n",
      "Iteration 69 loss nan\n",
      "Iteration 70 loss nan\n",
      "Iteration 71 loss nan\n",
      "Iteration 72 loss nan\n",
      "Iteration 73 loss nan\n",
      "Iteration 74 loss nan\n",
      "Iteration 75 loss nan\n",
      "Iteration 76 loss nan\n",
      "Iteration 77 loss nan\n",
      "Iteration 78 loss nan\n",
      "Iteration 79 loss nan\n",
      "Iteration 80 loss nan\n",
      "Iteration 81 loss nan\n",
      "Iteration 82 loss nan\n",
      "Iteration 83 loss nan\n",
      "Iteration 84 loss nan\n",
      "Iteration 85 loss nan\n",
      "Iteration 86 loss nan\n",
      "Iteration 87 loss nan\n",
      "Iteration 88 loss nan\n",
      "Iteration 89 loss nan\n",
      "Iteration 90 loss nan\n",
      "Iteration 91 loss nan\n",
      "Iteration 92 loss nan\n",
      "Iteration 93 loss nan\n",
      "Iteration 94 loss nan\n",
      "Iteration 95 loss nan\n",
      "Iteration 96 loss nan\n",
      "Iteration 97 loss nan\n",
      "Iteration 98 loss nan\n",
      "Iteration 99 loss nan\n",
      "Iteration 100 loss nan\n",
      "Iteration 101 loss nan\n",
      "Iteration 102 loss nan\n",
      "Iteration 103 loss nan\n",
      "Iteration 104 loss nan\n",
      "Iteration 105 loss nan\n",
      "Iteration 106 loss nan\n",
      "Iteration 107 loss nan\n",
      "Iteration 108 loss nan\n",
      "Iteration 109 loss nan\n",
      "Iteration 110 loss nan\n",
      "Iteration 111 loss nan\n",
      "Iteration 112 loss nan\n",
      "Iteration 113 loss nan\n",
      "Iteration 114 loss nan\n",
      "Iteration 115 loss nan\n",
      "Iteration 116 loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 117 loss nan\n",
      "Iteration 118 loss nan\n",
      "Iteration 119 loss nan\n",
      "Iteration 120 loss nan\n",
      "Iteration 121 loss nan\n",
      "Iteration 122 loss nan\n",
      "Iteration 123 loss nan\n",
      "Iteration 124 loss nan\n",
      "Iteration 125 loss nan\n",
      "Iteration 126 loss nan\n",
      "Iteration 127 loss nan\n",
      "Iteration 128 loss nan\n",
      "Iteration 129 loss nan\n",
      "Iteration 130 loss nan\n",
      "Iteration 131 loss nan\n",
      "Iteration 132 loss nan\n",
      "Iteration 133 loss nan\n",
      "Iteration 134 loss nan\n",
      "Iteration 135 loss nan\n",
      "Iteration 136 loss nan\n",
      "Iteration 137 loss nan\n",
      "Iteration 138 loss nan\n",
      "Iteration 139 loss nan\n",
      "Iteration 140 loss nan\n",
      "Iteration 141 loss nan\n",
      "Iteration 142 loss nan\n",
      "Iteration 143 loss nan\n",
      "Iteration 144 loss nan\n",
      "Iteration 145 loss nan\n",
      "Iteration 146 loss nan\n",
      "Iteration 147 loss nan\n",
      "Iteration 148 loss nan\n",
      "Iteration 149 loss nan\n",
      "Iteration 150 loss nan\n",
      "Iteration 151 loss nan\n",
      "Iteration 152 loss nan\n",
      "Iteration 153 loss nan\n",
      "Iteration 154 loss nan\n"
     ]
    }
   ],
   "source": [
    "mnist = neuralNetwork()\n",
    "mnist.__class__.FCreluLayers = 2 #excluding the softmax\n",
    "mnist.__class__.softmax = True\n",
    "mnist.__class__.softmaxClasses = 10\n",
    "N = 64\n",
    "h1 = 24\n",
    "h2 = 16\n",
    "mnist.FullyConnectedreluLayers([N,h1,h2])\n",
    "mnist.softmaxLayer()\n",
    "\n",
    "# M = 10\n",
    "max_iter = 1000\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        x_this = X[order[i],:]\n",
    "#         print(type(x_this))\n",
    "        y_this = y[order[i],0]\n",
    "#         print(y_this)\n",
    "#         print(x_this.shape)\n",
    "#         print(np.ones(64).shape)\n",
    "#         np.squeeze(np.asarray(x_this)).T \n",
    "        ypred = mnist.feedForward(x_this)\n",
    "        # print(np.shape(ypred), np.zeros(10))\n",
    "        \n",
    "        loss_this_iter += mnist.calcLoss(ypred, y_this)\n",
    "        mnist.backprop(ypred,y_this)\n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# r = np.array([1,0,0])\n",
    "# logits = np.array([0.6,0,0.5])\n",
    "# logits_for_answers = logits[np.arange(len(logits)),r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.matrix(np.zeros(5)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual\n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    print(yactual)\n",
    "    l = np.log(ypred)\n",
    "    print(l)\n",
    "    return - np.sum(l*yactual)/yactual.shape[0]\n",
    "\n",
    "crossEntropy(np.array([1.6,3.4,1.4,4]), 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "delta_cross_entropy(np.array([4,3,2,1]), np.array([3,4,12,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_der(np.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
