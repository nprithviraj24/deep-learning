{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RELU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_der(z):\n",
    "    return 1.*(z>0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_der(x):\n",
    "    prob = softmax(x)\n",
    "#     print(np.shape(prob))\n",
    "    r = np.matmul(prob.T, (np.ones_like(prob)-prob))\n",
    "#     print(np.shape(r))\n",
    "    return r\n",
    "\n",
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual    \n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    l = np.log(ypred)\n",
    "    return -np.sum(l.T*yactual)\n",
    "\n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y.T*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "\n",
    "\n",
    "\n",
    "class neuralNetwork():    \n",
    "    FCreluLayers = 0  \n",
    "    Weights = []\n",
    "    Biases = []\n",
    "    softmax = False\n",
    "    softmaxClasses = 0\n",
    "    loss = []\n",
    "    delta = [[]];    dW = [[]];    db = [[]]\n",
    "    act = []\n",
    "    logits = [[]]\n",
    "    def FullyConnectedreluLayers(self, values):\n",
    "        assert len(values) == self.FCreluLayers+1, \"Number of fully connected layers are not the same.\"\n",
    "        self.Weights.append([])\n",
    "        self.Biases.append([])\n",
    "        for i in range(1, self.FCreluLayers+1):            \n",
    "            self.Weights.append(np.random.normal(0, 0.1, ([values[i-1], values[i]])))\n",
    "            self.Biases.append(np.random.normal(0, 0.1, ([values[i], 1])))\n",
    "            self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "               \n",
    "    \n",
    "    def softmaxLayer(self):\n",
    "        previous = self.Weights[-1].shape[1]\n",
    "        self.Weights.append(np.random.normal(0,0.1,[previous, self.softmaxClasses]))\n",
    "        self.Biases.append(np.random.normal(0, 0.1, (self.softmaxClasses, 1)))\n",
    "        self.delta.append([]);    self.dW.append([]);    self.db.append([])\n",
    "        \n",
    "    def calcLoss(self, ypred, yactual):\n",
    "        if self.softmax == True:\n",
    "            return crossEntropy(ypred,yactual, self.softmaxClasses)\n",
    "    \n",
    "    def feedForward(self,X):\n",
    "        self.act.append(X.T)\n",
    "        for i in range(1, self.FCreluLayers+1):\n",
    "#             print(\"weights shape: \", np.shape(self.Weights[i].T), \" act: \", np.shape(self.act[i-1]))\n",
    "            self.logits.append(np.matmul(self.Weights[i].T, self.act[i-1]) + self.Biases[i])\n",
    "            self.act.append( RELU(self.logits[-1]) )\n",
    "\n",
    "        if self.softmax == True:\n",
    "            self.logits.append(np.matmul(self.Weights[-1].T, self.act[-1]) + self.Biases[-1])\n",
    "            self.act.append(softmax(self.logits[-1]))\n",
    "\n",
    "#         print(\"Weights: \", [np.shape(self.Weights[i]) for i in range(len(self.Weights))])\n",
    "#         print(\"act shapes: \", [np.shape(self.act[i]) for i in range(len(self.act))])\n",
    "#         print(\"logits shapes: \", [np.shape(self.logits[i]) for i in range(len(self.logits))])\n",
    "        return self.act[-1]\n",
    "    \n",
    "    def backprop(self, ypred, yact):\n",
    "        L = len(self.delta)-1\n",
    "        yact = oneHotEncode(yact, self.softmaxClasses)\n",
    "#         print(yact)\n",
    "        self.delta[L] = delta_cross_entropy(ypred,yact)\n",
    "#         print(\"delta_cross_entropy: \", np.sum(self.delta[L]))\n",
    "#         self.delta[L] = np.subtract(yact, ypred)\n",
    "#         print(np.shape(self.delta[L]))\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L].T\n",
    "\n",
    "        WxD = self.Weights[L] * self.delta[L]\n",
    "        self.delta[L-1] = np.multiply(softmax_der(self.logits[L-1]), WxD.T)\n",
    "        \n",
    "        L = 2\n",
    "\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L]\n",
    "        bsd = self.Weights[L]* self.delta[L].T\n",
    "#         print(\"weights: \", np.shape(self.Weights[L]), \" delta: \", np.shape(self.delta[L]))\n",
    "        reluOut = relu_der(self.logits[L-1])\n",
    "#         print(\"bsd: \", np.shape(bsd))\n",
    "#         print(\" logit to reul shape: \", np.shape(self.logits[L-1]))\n",
    "        self.delta[L-1] = np.multiply(reluOut, bsd)\n",
    "#         print(\"new delta: \", np.shape(self.delta[L-1]))\n",
    "        \n",
    "        \n",
    "        L = 1        \n",
    "\n",
    "        self.db[L] = self.delta[L].copy()\n",
    "        self.dW[L] = self.act[L-1] * self.delta[L].T\n",
    "            \n",
    "        alpha = 0.01\n",
    "        for l in range(1,len(self.Weights)):            \n",
    "            self.Weights[l] = self.Weights[l] - alpha * (-1*self.dW[l])\n",
    "            self.Biases[l] = self.Biases[l] - alpha * (-1*self.db[l])\n",
    "# mnist.loss.append(mnist.calcLoss(ypred, yt))\n",
    "# print(mnist.loss[-1])\n",
    "\n",
    "# print([np.shape(mnist.Weights[i]) for i in range(len(mnist.Weights))])  \n",
    "# print(mnist.feedForward(np.matrix([0,1,2,3,4,5]), 3 ))\n",
    "# mnist.layerWeights[-1].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing digits:  ['data', 'target', 'target_names', 'images', 'DESCR']\n",
      "Number of samples:  1797\n",
      "Size of each sample:  64\n",
      "Listing target names:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "number of sample:  1797  each sample:  64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(\"listing digits: \", list(digits))\n",
    "print(\"Number of samples: \", digits.data.shape[0])\n",
    "print(\"Size of each sample: \", digits.data.shape[1])\n",
    "print(\"Listing target names: \", list(digits.target_names))\n",
    "\n",
    "y = np.matrix(digits.target).T\n",
    "X = np.matrix(digits.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "print(\"number of sample: \",M, \" each sample: \",N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):    \n",
    "    return X/255\n",
    "\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at random index 3: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Image at random index 3: \")\n",
    "import matplotlib.pyplot as plt \n",
    "plt.matshow(digits.images[3]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 145944.650694\n",
      "Iteration 1 loss 146049.000659\n",
      "Iteration 2 loss 146122.038266\n",
      "Iteration 3 loss 146331.173656\n",
      "Iteration 4 loss 146805.606024\n",
      "Iteration 5 loss 147913.330356\n",
      "Iteration 6 loss 150327.666403\n",
      "Iteration 7 loss 155553.783121\n",
      "Iteration 8 loss 166160.803033\n",
      "Iteration 9 loss 185119.181665\n",
      "Iteration 10 loss 214047.660758\n",
      "Iteration 11 loss 251714.505749\n",
      "Iteration 12 loss 297178.750076\n",
      "Iteration 13 loss 349650.048678\n",
      "Iteration 14 loss 409082.953135\n",
      "Iteration 15 loss 475762.028403\n",
      "Iteration 16 loss 550152.833741\n",
      "Iteration 17 loss 632777.263419\n",
      "Iteration 18 loss 723324.186886\n",
      "Iteration 19 loss 822832.496913\n",
      "Iteration 20 loss 931637.444480\n",
      "Iteration 21 loss 1049660.068416\n",
      "Iteration 22 loss 1177564.604203\n",
      "Iteration 23 loss 1315735.723830\n",
      "Iteration 24 loss 1464430.008528\n",
      "Iteration 25 loss 1624388.307059\n",
      "Iteration 26 loss 1795163.296225\n",
      "Iteration 27 loss 1978292.327578\n",
      "Iteration 28 loss 2172632.716823\n",
      "Iteration 29 loss 2379598.930777\n",
      "Iteration 30 loss 2599969.274304\n",
      "Iteration 31 loss 2833462.472491\n",
      "Iteration 32 loss 3079895.127953\n",
      "Iteration 33 loss 3340672.457142\n",
      "Iteration 34 loss 3615445.169946\n",
      "Iteration 35 loss 3904011.379832\n",
      "Iteration 36 loss 4208662.346162\n",
      "Iteration 37 loss 4529361.579629\n",
      "Iteration 38 loss 4864700.046690\n",
      "Iteration 39 loss 5216993.140384\n",
      "Iteration 40 loss 5585475.562236\n",
      "Iteration 41 loss 5969499.654455\n",
      "Iteration 42 loss 6373437.913200\n",
      "Iteration 43 loss 6793226.804401\n",
      "Iteration 44 loss 7230854.422651\n",
      "Iteration 45 loss 7689012.970398\n",
      "Iteration 46 loss 8163451.837482\n",
      "Iteration 47 loss 8658531.836388\n",
      "Iteration 48 loss 9173553.540865\n",
      "Iteration 49 loss 9706472.809630\n",
      "Iteration 50 loss 10260488.104938\n",
      "Iteration 51 loss 10835996.251115\n",
      "Iteration 52 loss 11432580.303053\n",
      "Iteration 53 loss 12049426.321607\n",
      "Iteration 54 loss 12689376.447304\n",
      "Iteration 55 loss 13350229.585765\n",
      "Iteration 56 loss 14035035.882277\n",
      "Iteration 57 loss 14741938.507484\n",
      "Iteration 58 loss 15471939.894019\n",
      "Iteration 59 loss 16225766.415300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "/home/prithvi/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60 loss nan\n",
      "Iteration 61 loss nan\n",
      "Iteration 62 loss nan\n",
      "Iteration 63 loss nan\n",
      "Iteration 64 loss nan\n",
      "Iteration 65 loss nan\n",
      "Iteration 66 loss nan\n",
      "Iteration 67 loss nan\n",
      "Iteration 68 loss nan\n",
      "Iteration 69 loss nan\n",
      "Iteration 70 loss nan\n",
      "Iteration 71 loss nan\n",
      "Iteration 72 loss nan\n",
      "Iteration 73 loss nan\n",
      "Iteration 74 loss nan\n",
      "Iteration 75 loss nan\n",
      "Iteration 76 loss nan\n",
      "Iteration 77 loss nan\n",
      "Iteration 78 loss nan\n",
      "Iteration 79 loss nan\n",
      "Iteration 80 loss nan\n",
      "Iteration 81 loss nan\n",
      "Iteration 82 loss nan\n",
      "Iteration 83 loss nan\n",
      "Iteration 84 loss nan\n",
      "Iteration 85 loss nan\n",
      "Iteration 86 loss nan\n",
      "Iteration 87 loss nan\n",
      "Iteration 88 loss nan\n",
      "Iteration 89 loss nan\n",
      "Iteration 90 loss nan\n",
      "Iteration 91 loss nan\n",
      "Iteration 92 loss nan\n",
      "Iteration 93 loss nan\n",
      "Iteration 94 loss nan\n",
      "Iteration 95 loss nan\n",
      "Iteration 96 loss nan\n",
      "Iteration 97 loss nan\n",
      "Iteration 98 loss nan\n",
      "Iteration 99 loss nan\n",
      "Iteration 100 loss nan\n",
      "Iteration 101 loss nan\n",
      "Iteration 102 loss nan\n",
      "Iteration 103 loss nan\n",
      "Iteration 104 loss nan\n",
      "Iteration 105 loss nan\n",
      "Iteration 106 loss nan\n",
      "Iteration 107 loss nan\n",
      "Iteration 108 loss nan\n",
      "Iteration 109 loss nan\n",
      "Iteration 110 loss nan\n",
      "Iteration 111 loss nan\n",
      "Iteration 112 loss nan\n",
      "Iteration 113 loss nan\n",
      "Iteration 114 loss nan\n",
      "Iteration 115 loss nan\n",
      "Iteration 116 loss nan\n",
      "Iteration 117 loss nan\n",
      "Iteration 118 loss nan\n",
      "Iteration 119 loss nan\n",
      "Iteration 120 loss nan\n",
      "Iteration 121 loss nan\n",
      "Iteration 122 loss nan\n",
      "Iteration 123 loss nan\n",
      "Iteration 124 loss nan\n",
      "Iteration 125 loss nan\n",
      "Iteration 126 loss nan\n",
      "Iteration 127 loss nan\n",
      "Iteration 128 loss nan\n",
      "Iteration 129 loss nan\n",
      "Iteration 130 loss nan\n",
      "Iteration 131 loss nan\n",
      "Iteration 132 loss nan\n",
      "Iteration 133 loss nan\n",
      "Iteration 134 loss nan\n",
      "Iteration 135 loss nan\n",
      "Iteration 136 loss nan\n",
      "Iteration 137 loss nan\n",
      "Iteration 138 loss nan\n",
      "Iteration 139 loss nan\n",
      "Iteration 140 loss nan\n",
      "Iteration 141 loss nan\n",
      "Iteration 142 loss nan\n",
      "Iteration 143 loss nan\n",
      "Iteration 144 loss nan\n",
      "Iteration 145 loss nan\n",
      "Iteration 146 loss nan\n",
      "Iteration 147 loss nan\n",
      "Iteration 148 loss nan\n"
     ]
    }
   ],
   "source": [
    "mnist = neuralNetwork()\n",
    "mnist.__class__.FCreluLayers = 2 #excluding the softmax\n",
    "mnist.__class__.softmax = True\n",
    "mnist.__class__.softmaxClasses = 10\n",
    "N = 64\n",
    "h1 = 24\n",
    "h2 = 16\n",
    "mnist.FullyConnectedreluLayers([N,h1,h2])\n",
    "mnist.softmaxLayer()\n",
    "\n",
    "# M = 10\n",
    "max_iter = 1000\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        x_this = X[order[i],:]\n",
    "#         print(type(x_this))\n",
    "        y_this = y[order[i],0]\n",
    "#         print(y_this)\n",
    "#         print(x_this.shape)\n",
    "#         print(np.ones(64).shape)\n",
    "#         np.squeeze(np.asarray(x_this)).T \n",
    "        ypred = mnist.feedForward(x_this)\n",
    "        # print(np.shape(ypred), np.zeros(10))\n",
    "        \n",
    "        loss_this_iter += mnist.calcLoss(ypred, y_this)\n",
    "        mnist.backprop(ypred,y_this)\n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# r = np.array([1,0,0])\n",
    "# logits = np.array([0.6,0,0.5])\n",
    "# logits_for_answers = logits[np.arange(len(logits)),r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.matrix(np.zeros(5)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual\n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    print(yactual)\n",
    "    l = np.log(ypred)\n",
    "    print(l)\n",
    "    return - np.sum(l*yactual)/yactual.shape[0]\n",
    "\n",
    "crossEntropy(np.array([1.6,3.4,1.4,4]), 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "delta_cross_entropy(np.array([4,3,2,1]), np.array([3,4,12,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_der(np.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
