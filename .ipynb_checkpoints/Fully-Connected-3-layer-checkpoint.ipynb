{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return XX\n",
    "\n",
    "XX = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 5\n",
    "h1 = 6\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 378.306881\n",
      "Iteration 1 loss 376.171569\n",
      "Iteration 2 loss 375.629844\n",
      "Iteration 3 loss 374.579677\n",
      "Iteration 4 loss 372.561964\n",
      "Iteration 5 loss 369.130907\n",
      "Iteration 6 loss 360.610839\n",
      "Iteration 7 loss 348.432113\n",
      "Iteration 8 loss 318.941665\n",
      "Iteration 9 loss 269.429416\n",
      "Iteration 10 loss 209.540712\n",
      "Iteration 11 loss 159.182504\n",
      "Iteration 12 loss 122.985221\n",
      "Iteration 13 loss 98.599722\n",
      "Iteration 14 loss 82.671303\n",
      "Iteration 15 loss 71.726888\n",
      "Iteration 16 loss 63.946665\n",
      "Iteration 17 loss 58.336312\n",
      "Iteration 18 loss 54.067648\n",
      "Iteration 19 loss 50.673430\n",
      "Iteration 20 loss 48.100940\n",
      "Iteration 21 loss 45.886620\n",
      "Iteration 22 loss 44.005641\n",
      "Iteration 23 loss 42.457702\n",
      "Iteration 24 loss 40.961580\n",
      "Iteration 25 loss 39.889138\n",
      "Iteration 26 loss 38.845684\n",
      "Iteration 27 loss 37.951864\n",
      "Iteration 28 loss 37.028534\n",
      "Iteration 29 loss 36.424398\n",
      "Iteration 30 loss 35.790592\n",
      "Iteration 31 loss 35.161696\n",
      "Iteration 32 loss 34.607348\n",
      "Iteration 33 loss 34.170126\n",
      "Iteration 34 loss 33.627383\n",
      "Iteration 35 loss 33.241154\n",
      "Iteration 36 loss 32.849934\n",
      "Iteration 37 loss 32.551997\n",
      "Iteration 38 loss 32.197873\n",
      "Iteration 39 loss 31.900163\n",
      "Iteration 40 loss 31.490329\n",
      "Iteration 41 loss 31.360476\n",
      "Iteration 42 loss 30.974695\n",
      "Iteration 43 loss 30.780444\n",
      "Iteration 44 loss 30.472184\n",
      "Iteration 45 loss 30.371593\n",
      "Iteration 46 loss 30.099103\n",
      "Iteration 47 loss 29.918292\n",
      "Iteration 48 loss 29.763521\n",
      "Iteration 49 loss 29.531827\n",
      "Iteration 50 loss 29.397844\n",
      "Iteration 51 loss 29.253140\n",
      "Iteration 52 loss 29.063324\n",
      "Iteration 53 loss 28.880947\n",
      "Iteration 54 loss 28.745274\n",
      "Iteration 55 loss 28.558716\n",
      "Iteration 56 loss 28.387985\n",
      "Iteration 57 loss 28.416445\n",
      "Iteration 58 loss 28.193557\n",
      "Iteration 59 loss 28.226382\n",
      "Iteration 60 loss 27.977991\n",
      "Iteration 61 loss 27.833353\n",
      "Iteration 62 loss 27.803349\n",
      "Iteration 63 loss 27.573319\n",
      "Iteration 64 loss 27.625777\n",
      "Iteration 65 loss 27.298444\n",
      "Iteration 66 loss 27.291038\n",
      "Iteration 67 loss 27.375543\n",
      "Iteration 68 loss 27.181633\n",
      "Iteration 69 loss 27.277609\n",
      "Iteration 70 loss 27.007181\n",
      "Iteration 71 loss 26.880002\n",
      "Iteration 72 loss 26.786025\n",
      "Iteration 73 loss 26.707532\n",
      "Iteration 74 loss 26.489957\n",
      "Iteration 75 loss 26.325417\n",
      "Iteration 76 loss 26.275198\n",
      "Iteration 77 loss 26.171765\n",
      "Iteration 78 loss 26.265521\n",
      "Iteration 79 loss 26.076955\n",
      "Iteration 80 loss 26.142174\n",
      "Iteration 81 loss 25.888549\n",
      "Iteration 82 loss 25.709564\n",
      "Iteration 83 loss 25.520692\n",
      "Iteration 84 loss 25.609720\n",
      "Iteration 85 loss 25.443321\n",
      "Iteration 86 loss 25.504947\n",
      "Iteration 87 loss 25.292261\n",
      "Iteration 88 loss 25.190465\n",
      "Iteration 89 loss 25.127224\n",
      "Iteration 90 loss 24.996997\n",
      "Iteration 91 loss 24.731977\n",
      "Iteration 92 loss 24.843805\n",
      "Iteration 93 loss 24.626442\n",
      "Iteration 94 loss 24.561601\n",
      "Iteration 95 loss 24.328270\n",
      "Iteration 96 loss 24.476303\n",
      "Iteration 97 loss 24.373480\n",
      "Iteration 98 loss 24.281929\n",
      "Iteration 99 loss 24.090018\n",
      "Iteration 100 loss 24.096191\n",
      "Iteration 101 loss 23.892621\n",
      "Iteration 102 loss 23.691750\n",
      "Iteration 103 loss 23.759141\n",
      "Iteration 104 loss 23.695333\n",
      "Iteration 105 loss 23.631122\n",
      "Iteration 106 loss 23.444479\n",
      "Iteration 107 loss 23.417293\n",
      "Iteration 108 loss 23.374512\n",
      "Iteration 109 loss 23.251704\n",
      "Iteration 110 loss 23.109272\n",
      "Iteration 111 loss 23.094706\n",
      "Iteration 112 loss 22.953655\n",
      "Iteration 113 loss 22.882508\n",
      "Iteration 114 loss 22.842062\n",
      "Iteration 115 loss 22.619869\n",
      "Iteration 116 loss 22.710449\n",
      "Iteration 117 loss 22.589487\n",
      "Iteration 118 loss 22.508793\n",
      "Iteration 119 loss 22.412112\n",
      "Iteration 120 loss 22.282958\n",
      "Iteration 121 loss 22.234001\n",
      "Iteration 122 loss 22.167209\n",
      "Iteration 123 loss 21.991600\n",
      "Iteration 124 loss 22.022602\n",
      "Iteration 125 loss 21.897775\n",
      "Iteration 126 loss 21.898983\n",
      "Iteration 127 loss 21.827363\n",
      "Iteration 128 loss 21.701196\n",
      "Iteration 129 loss 21.460156\n",
      "Iteration 130 loss 21.553155\n",
      "Iteration 131 loss 21.369179\n",
      "Iteration 132 loss 21.406153\n",
      "Iteration 133 loss 21.365114\n",
      "Iteration 134 loss 21.282812\n",
      "Iteration 135 loss 21.159068\n",
      "Iteration 136 loss 21.122449\n",
      "Iteration 137 loss 21.019149\n",
      "Iteration 138 loss 20.923716\n",
      "Iteration 139 loss 20.900301\n",
      "Iteration 140 loss 20.923130\n",
      "Iteration 141 loss 20.810849\n",
      "Iteration 142 loss 20.622901\n",
      "Iteration 143 loss 20.654001\n",
      "Iteration 144 loss 20.553666\n",
      "Iteration 145 loss 20.456120\n",
      "Iteration 146 loss 20.495833\n",
      "Iteration 147 loss 20.391012\n",
      "Iteration 148 loss 20.372641\n",
      "Iteration 149 loss 20.288466\n",
      "Iteration 150 loss 20.065967\n",
      "Iteration 151 loss 20.200375\n",
      "Iteration 152 loss 20.113273\n",
      "Iteration 153 loss 19.930478\n",
      "Iteration 154 loss 19.950910\n",
      "Iteration 155 loss 19.885997\n",
      "Iteration 156 loss 19.813164\n",
      "Iteration 157 loss 19.745864\n",
      "Iteration 158 loss 19.647148\n",
      "Iteration 159 loss 19.613453\n",
      "Iteration 160 loss 19.570503\n",
      "Iteration 161 loss 19.539333\n",
      "Iteration 162 loss 19.402822\n",
      "Iteration 163 loss 19.381455\n",
      "Iteration 164 loss 19.183664\n",
      "Iteration 165 loss 19.288044\n",
      "Iteration 166 loss 19.203059\n",
      "Iteration 167 loss 19.141730\n",
      "Iteration 168 loss 19.049420\n",
      "Iteration 169 loss 19.048777\n",
      "Iteration 170 loss 19.007625\n",
      "Iteration 171 loss 18.879134\n",
      "Iteration 172 loss 18.863263\n",
      "Iteration 173 loss 18.766544\n",
      "Iteration 174 loss 18.715197\n",
      "Iteration 175 loss 18.653183\n",
      "Iteration 176 loss 18.588188\n",
      "Iteration 177 loss 18.362164\n",
      "Iteration 178 loss 18.476531\n",
      "Iteration 179 loss 18.407375\n",
      "Iteration 180 loss 18.209041\n",
      "Iteration 181 loss 18.150724\n",
      "Iteration 182 loss 18.187695\n",
      "Iteration 183 loss 18.152540\n",
      "Iteration 184 loss 18.049452\n",
      "Iteration 185 loss 17.976954\n",
      "Iteration 186 loss 17.915790\n",
      "Iteration 187 loss 17.811191\n",
      "Iteration 188 loss 17.649768\n",
      "Iteration 189 loss 17.578753\n",
      "Iteration 190 loss 17.636109\n",
      "Iteration 191 loss 17.580769\n",
      "Iteration 192 loss 17.329419\n",
      "Iteration 193 loss 17.358699\n",
      "Iteration 194 loss 17.400575\n",
      "Iteration 195 loss 17.265931\n",
      "Iteration 196 loss 17.238742\n",
      "Iteration 197 loss 17.096184\n",
      "Iteration 198 loss 17.100982\n",
      "Iteration 199 loss 17.002923\n",
      "Iteration 200 loss 16.974332\n",
      "Iteration 201 loss 16.894813\n",
      "Iteration 202 loss 16.764317\n",
      "Iteration 203 loss 16.616998\n",
      "Iteration 204 loss 16.586689\n",
      "Iteration 205 loss 16.586470\n",
      "Iteration 206 loss 16.491902\n",
      "Iteration 207 loss 16.417957\n",
      "Iteration 208 loss 16.261294\n",
      "Iteration 209 loss 16.073728\n",
      "Iteration 210 loss 16.229888\n",
      "Iteration 211 loss 16.066480\n",
      "Iteration 212 loss 16.030175\n",
      "Iteration 213 loss 15.936378\n",
      "Iteration 214 loss 15.829526\n",
      "Iteration 215 loss 15.805414\n",
      "Iteration 216 loss 15.539939\n",
      "Iteration 217 loss 15.671252\n",
      "Iteration 218 loss 15.523400\n",
      "Iteration 219 loss 15.473308\n",
      "Iteration 220 loss 15.271516\n",
      "Iteration 221 loss 15.294175\n",
      "Iteration 222 loss 15.116624\n",
      "Iteration 223 loss 15.026981\n",
      "Iteration 224 loss 14.996931\n",
      "Iteration 225 loss 15.036476\n",
      "Iteration 226 loss 14.844360\n",
      "Iteration 227 loss 14.728532\n",
      "Iteration 228 loss 14.761860\n",
      "Iteration 229 loss 14.472508\n",
      "Iteration 230 loss 14.606834\n",
      "Iteration 231 loss 14.503000\n",
      "Iteration 232 loss 14.196990\n",
      "Iteration 233 loss 14.325905\n",
      "Iteration 234 loss 14.223922\n",
      "Iteration 235 loss 14.133040\n",
      "Iteration 236 loss 13.887966\n",
      "Iteration 237 loss 13.828888\n",
      "Iteration 238 loss 13.826847\n",
      "Iteration 239 loss 13.893080\n",
      "Iteration 240 loss 13.817791\n",
      "Iteration 241 loss 13.660970\n",
      "Iteration 242 loss 13.569360\n",
      "Iteration 243 loss 13.557116\n",
      "Iteration 244 loss 13.468323\n",
      "Iteration 245 loss 13.404629\n",
      "Iteration 246 loss 13.270414\n",
      "Iteration 247 loss 13.279109\n",
      "Iteration 248 loss 13.121495\n",
      "Iteration 249 loss 13.110662\n",
      "Iteration 250 loss 13.033153\n",
      "Iteration 251 loss 12.962722\n",
      "Iteration 252 loss 12.896446\n",
      "Iteration 253 loss 12.701054\n",
      "Iteration 254 loss 12.761410\n",
      "Iteration 255 loss 12.697842\n",
      "Iteration 256 loss 12.567026\n",
      "Iteration 257 loss 12.566931\n",
      "Iteration 258 loss 12.441480\n",
      "Iteration 259 loss 12.349865\n",
      "Iteration 260 loss 12.059437\n",
      "Iteration 261 loss 12.242309\n",
      "Iteration 262 loss 12.219803\n",
      "Iteration 263 loss 12.141380\n",
      "Iteration 264 loss 12.055792\n",
      "Iteration 265 loss 12.030920\n",
      "Iteration 266 loss 11.949410\n",
      "Iteration 267 loss 11.832635\n",
      "Iteration 268 loss 11.819220\n",
      "Iteration 269 loss 11.774596\n",
      "Iteration 270 loss 11.712987\n",
      "Iteration 271 loss 11.527172\n",
      "Iteration 272 loss 11.597033\n",
      "Iteration 273 loss 11.495869\n",
      "Iteration 274 loss 11.474395\n",
      "Iteration 275 loss 11.317049\n",
      "Iteration 276 loss 11.350101\n",
      "Iteration 277 loss 11.257918\n",
      "Iteration 278 loss 11.117081\n",
      "Iteration 279 loss 11.123802\n",
      "Iteration 280 loss 11.045409\n",
      "Iteration 281 loss 11.064727\n",
      "Iteration 282 loss 10.970388\n",
      "Iteration 283 loss 10.817153\n",
      "Iteration 284 loss 10.851354\n",
      "Iteration 285 loss 10.821115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 286 loss 10.779198\n",
      "Iteration 287 loss 10.568191\n",
      "Iteration 288 loss 10.580141\n",
      "Iteration 289 loss 10.576620\n",
      "Iteration 290 loss 10.531476\n",
      "Iteration 291 loss 10.455175\n",
      "Iteration 292 loss 10.435509\n",
      "Iteration 293 loss 10.364306\n",
      "Iteration 294 loss 10.228898\n",
      "Iteration 295 loss 10.268953\n",
      "Iteration 296 loss 10.174840\n",
      "Iteration 297 loss 10.124904\n",
      "Iteration 298 loss 10.031175\n",
      "Iteration 299 loss 9.957024\n",
      "Iteration 300 loss 9.932502\n",
      "Iteration 301 loss 9.925031\n",
      "Iteration 302 loss 9.880664\n",
      "Iteration 303 loss 9.726750\n",
      "Iteration 304 loss 9.790531\n",
      "Iteration 305 loss 9.750516\n",
      "Iteration 306 loss 9.608154\n",
      "Iteration 307 loss 9.623615\n",
      "Iteration 308 loss 9.581094\n",
      "Iteration 309 loss 9.442535\n",
      "Iteration 310 loss 9.466988\n",
      "Iteration 311 loss 9.400089\n",
      "Iteration 312 loss 9.352625\n",
      "Iteration 313 loss 9.322280\n",
      "Iteration 314 loss 9.260649\n",
      "Iteration 315 loss 9.227946\n",
      "Iteration 316 loss 9.173039\n",
      "Iteration 317 loss 9.095603\n",
      "Iteration 318 loss 9.065046\n",
      "Iteration 319 loss 9.022733\n",
      "Iteration 320 loss 8.982787\n",
      "Iteration 321 loss 8.928960\n",
      "Iteration 322 loss 8.849084\n",
      "Iteration 323 loss 8.797126\n",
      "Iteration 324 loss 8.768209\n",
      "Iteration 325 loss 8.720249\n",
      "Iteration 326 loss 8.700672\n",
      "Iteration 327 loss 8.606269\n",
      "Iteration 328 loss 8.581480\n",
      "Iteration 329 loss 8.511000\n",
      "Iteration 330 loss 8.428220\n",
      "Iteration 331 loss 8.438329\n",
      "Iteration 332 loss 8.327457\n",
      "Iteration 333 loss 8.181321\n",
      "Iteration 334 loss 8.242293\n",
      "Iteration 335 loss 8.260860\n",
      "Iteration 336 loss 8.175524\n",
      "Iteration 337 loss 7.959906\n",
      "Iteration 338 loss 8.072519\n",
      "Iteration 339 loss 7.890721\n",
      "Iteration 340 loss 7.997370\n",
      "Iteration 341 loss 7.917556\n",
      "Iteration 342 loss 7.865244\n",
      "Iteration 343 loss 7.814134\n",
      "Iteration 344 loss 7.771455\n",
      "Iteration 345 loss 7.763970\n",
      "Iteration 346 loss 7.695328\n",
      "Iteration 347 loss 7.602614\n",
      "Iteration 348 loss 7.558031\n",
      "Iteration 349 loss 7.548393\n",
      "Iteration 350 loss 7.469696\n",
      "Iteration 351 loss 7.481343\n",
      "Iteration 352 loss 7.362840\n",
      "Iteration 353 loss 7.363216\n",
      "Iteration 354 loss 7.318995\n",
      "Iteration 355 loss 7.214247\n",
      "Iteration 356 loss 7.251013\n",
      "Iteration 357 loss 7.154609\n",
      "Iteration 358 loss 7.108479\n",
      "Iteration 359 loss 7.096024\n",
      "Iteration 360 loss 7.009341\n",
      "Iteration 361 loss 6.969872\n",
      "Iteration 362 loss 6.917092\n",
      "Iteration 363 loss 6.905028\n",
      "Iteration 364 loss 6.702923\n",
      "Iteration 365 loss 6.774720\n",
      "Iteration 366 loss 6.771016\n",
      "Iteration 367 loss 6.715588\n",
      "Iteration 368 loss 6.674898\n",
      "Iteration 369 loss 6.637066\n",
      "Iteration 370 loss 6.503352\n",
      "Iteration 371 loss 6.559999\n",
      "Iteration 372 loss 6.494791\n",
      "Iteration 373 loss 6.425430\n",
      "Iteration 374 loss 6.396151\n",
      "Iteration 375 loss 6.363183\n",
      "Iteration 376 loss 6.289691\n",
      "Iteration 377 loss 6.246919\n",
      "Iteration 378 loss 6.193438\n",
      "Iteration 379 loss 6.151247\n",
      "Iteration 380 loss 6.153465\n",
      "Iteration 381 loss 6.061146\n",
      "Iteration 382 loss 6.045347\n",
      "Iteration 383 loss 5.986547\n",
      "Iteration 384 loss 5.902121\n",
      "Iteration 385 loss 5.913809\n",
      "Iteration 386 loss 5.900122\n",
      "Iteration 387 loss 5.794560\n",
      "Iteration 388 loss 5.786328\n",
      "Iteration 389 loss 5.734052\n",
      "Iteration 390 loss 5.681850\n",
      "Iteration 391 loss 5.681140\n",
      "Iteration 392 loss 5.609716\n",
      "Iteration 393 loss 5.573655\n",
      "Iteration 394 loss 5.532941\n",
      "Iteration 395 loss 5.490766\n",
      "Iteration 396 loss 5.448773\n",
      "Iteration 397 loss 5.386162\n",
      "Iteration 398 loss 5.385434\n",
      "Iteration 399 loss 5.309745\n",
      "Iteration 400 loss 5.313486\n",
      "Iteration 401 loss 5.245508\n",
      "Iteration 402 loss 5.184332\n",
      "Iteration 403 loss 5.166548\n",
      "Iteration 404 loss 5.136081\n",
      "Iteration 405 loss 5.074579\n",
      "Iteration 406 loss 5.062724\n",
      "Iteration 407 loss 4.929661\n",
      "Iteration 408 loss 4.994362\n",
      "Iteration 409 loss 4.922204\n",
      "Iteration 410 loss 4.886646\n",
      "Iteration 411 loss 4.848210\n",
      "Iteration 412 loss 4.710292\n",
      "Iteration 413 loss 4.799499\n",
      "Iteration 414 loss 4.725121\n",
      "Iteration 415 loss 4.658757\n",
      "Iteration 416 loss 4.627972\n",
      "Iteration 417 loss 4.588290\n",
      "Iteration 418 loss 4.514231\n",
      "Iteration 419 loss 4.496642\n",
      "Iteration 420 loss 4.464801\n",
      "Iteration 421 loss 4.394447\n",
      "Iteration 422 loss 4.341157\n",
      "Iteration 423 loss 4.320863\n",
      "Iteration 424 loss 4.211728\n",
      "Iteration 425 loss 4.268049\n",
      "Iteration 426 loss 4.183239\n",
      "Iteration 427 loss 4.142112\n",
      "Iteration 428 loss 4.078601\n",
      "Iteration 429 loss 3.992973\n",
      "Iteration 430 loss 3.988426\n",
      "Iteration 431 loss 3.912180\n",
      "Iteration 432 loss 3.877984\n",
      "Iteration 433 loss 3.802376\n",
      "Iteration 434 loss 3.713030\n",
      "Iteration 435 loss 3.649216\n",
      "Iteration 436 loss 3.656508\n",
      "Iteration 437 loss 3.583193\n",
      "Iteration 438 loss 3.548826\n",
      "Iteration 439 loss 3.485835\n",
      "Iteration 440 loss 3.409062\n",
      "Iteration 441 loss 3.378084\n",
      "Iteration 442 loss 3.305659\n",
      "Iteration 443 loss 3.246169\n",
      "Iteration 444 loss 3.248127\n",
      "Iteration 445 loss 3.155626\n",
      "Iteration 446 loss 3.134130\n",
      "Iteration 447 loss 3.033731\n",
      "Iteration 448 loss 3.045550\n",
      "Iteration 449 loss 3.023390\n",
      "Iteration 450 loss 2.960839\n",
      "Iteration 451 loss 2.929920\n",
      "Iteration 452 loss 2.841793\n",
      "Iteration 453 loss 2.883292\n",
      "Iteration 454 loss 2.805087\n",
      "Iteration 455 loss 2.787841\n",
      "Iteration 456 loss 2.762581\n",
      "Iteration 457 loss 2.730280\n",
      "Iteration 458 loss 2.681607\n",
      "Iteration 459 loss 2.654302\n",
      "Iteration 460 loss 2.633085\n",
      "Iteration 461 loss 2.595718\n",
      "Iteration 462 loss 2.579938\n",
      "Iteration 463 loss 2.499837\n",
      "Iteration 464 loss 2.507678\n",
      "Iteration 465 loss 2.503045\n",
      "Iteration 466 loss 2.465358\n",
      "Iteration 467 loss 2.434985\n",
      "Iteration 468 loss 2.425075\n",
      "Iteration 469 loss 2.379288\n",
      "Iteration 470 loss 2.373603\n",
      "Iteration 471 loss 2.335309\n",
      "Iteration 472 loss 2.335344\n",
      "Iteration 473 loss 2.309146\n",
      "Iteration 474 loss 2.284556\n",
      "Iteration 475 loss 2.268486\n",
      "Iteration 476 loss 2.245048\n",
      "Iteration 477 loss 2.222864\n",
      "Iteration 478 loss 2.209270\n",
      "Iteration 479 loss 2.176136\n",
      "Iteration 480 loss 2.168068\n",
      "Iteration 481 loss 2.154036\n",
      "Iteration 482 loss 2.135545\n",
      "Iteration 483 loss 2.097308\n",
      "Iteration 484 loss 2.095629\n",
      "Iteration 485 loss 2.069476\n",
      "Iteration 486 loss 2.057689\n",
      "Iteration 487 loss 2.048547\n",
      "Iteration 488 loss 2.023672\n",
      "Iteration 489 loss 2.014443\n",
      "Iteration 490 loss 1.994588\n",
      "Iteration 491 loss 1.973183\n",
      "Iteration 492 loss 1.952767\n",
      "Iteration 493 loss 1.949029\n",
      "Iteration 494 loss 1.932112\n",
      "Iteration 495 loss 1.917550\n",
      "Iteration 496 loss 1.906484\n",
      "Iteration 497 loss 1.879704\n",
      "Iteration 498 loss 1.877999\n",
      "Iteration 499 loss 1.871166\n",
      "Iteration 500 loss 1.853745\n",
      "Iteration 501 loss 1.832223\n",
      "Iteration 502 loss 1.813765\n",
      "Iteration 503 loss 1.809793\n",
      "Iteration 504 loss 1.793843\n",
      "Iteration 505 loss 1.789268\n",
      "Iteration 506 loss 1.770962\n",
      "Iteration 507 loss 1.742968\n",
      "Iteration 508 loss 1.755813\n",
      "Iteration 509 loss 1.736027\n",
      "Iteration 510 loss 1.719644\n",
      "Iteration 511 loss 1.713853\n",
      "Iteration 512 loss 1.704413\n",
      "Iteration 513 loss 1.690430\n",
      "Iteration 514 loss 1.683149\n",
      "Iteration 515 loss 1.669719\n",
      "Iteration 516 loss 1.635264\n",
      "Iteration 517 loss 1.648550\n",
      "Iteration 518 loss 1.634976\n",
      "Iteration 519 loss 1.627222\n",
      "Iteration 520 loss 1.619048\n",
      "Iteration 521 loss 1.597221\n",
      "Iteration 522 loss 1.598064\n",
      "Iteration 523 loss 1.587724\n",
      "Iteration 524 loss 1.575275\n",
      "Iteration 525 loss 1.561538\n",
      "Iteration 526 loss 1.559805\n",
      "Iteration 527 loss 1.547560\n",
      "Iteration 528 loss 1.535181\n",
      "Iteration 529 loss 1.531301\n",
      "Iteration 530 loss 1.519740\n",
      "Iteration 531 loss 1.511640\n",
      "Iteration 532 loss 1.499106\n",
      "Iteration 533 loss 1.488057\n",
      "Iteration 534 loss 1.485767\n",
      "Iteration 535 loss 1.477653\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Use mini-batch size 1\n",
    "\n",
    "alpha = 0.01\n",
    "max_iter = 1000\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            a.append(act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss(y_this, a[L][0,0])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step\n",
    "\n",
    "        delta[L] = a[L] - y_this\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                delta[l-1] = np.multiply(actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
