{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at index 3: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of data:  ['data', 'target', 'target_names', 'images', 'DESCR']\n",
      "Label of an image:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "y = np.matrix(digits.target).T\n",
    "X = np.matrix(digits.data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "M = X_train.shape[0]\n",
    "N = X_train.shape[1]\n",
    "\n",
    "print(\"Image at index 3: \")\n",
    "import matplotlib.pyplot as plt \n",
    "plt.matshow(digits.images[3]) \n",
    "plt.show()\n",
    "\n",
    "attributes = list(digits)\n",
    "print(\"Attributes of data: \", attributes)\n",
    "print(\"Label of an image: \", digits.target[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 2888.894581\n",
      "Iteration 1 loss 2857.868028\n",
      "Iteration 2 loss 2824.410881\n",
      "Iteration 3 loss 2795.267401\n",
      "Iteration 4 loss 2763.295420\n",
      "Iteration 5 loss 2737.856998\n",
      "Iteration 6 loss 2707.792653\n",
      "Iteration 7 loss 2680.391120\n",
      "Iteration 8 loss 2653.603659\n",
      "Iteration 9 loss 2627.867402\n",
      "Iteration 10 loss 2600.422070\n",
      "Iteration 11 loss 2574.815153\n",
      "Iteration 12 loss 2549.687100\n",
      "Iteration 13 loss 2523.776674\n",
      "Iteration 14 loss 2498.766101\n",
      "Iteration 15 loss 2475.671592\n",
      "Iteration 16 loss 2451.858567\n",
      "Iteration 17 loss 2427.981073\n",
      "Iteration 18 loss 2405.024751\n",
      "Iteration 19 loss 2381.743802\n",
      "Iteration 20 loss 2356.892950\n",
      "Iteration 21 loss 2336.937758\n",
      "Iteration 22 loss 2313.959935\n",
      "Iteration 23 loss 2292.386842\n",
      "Iteration 24 loss 2270.240024\n",
      "Iteration 25 loss 2250.118815\n",
      "Iteration 26 loss 2228.919814\n",
      "Iteration 27 loss 2207.879187\n",
      "Iteration 28 loss 2184.679905\n",
      "Iteration 29 loss 2163.846567\n",
      "Iteration 30 loss 2142.241244\n",
      "Iteration 31 loss 2121.068703\n",
      "Iteration 32 loss 2103.646203\n",
      "Iteration 33 loss 2083.164993\n",
      "Iteration 34 loss 2061.038129\n",
      "Iteration 35 loss 2043.435578\n",
      "Iteration 36 loss 2022.228947\n",
      "Iteration 37 loss 2003.369814\n",
      "Iteration 38 loss 1983.829430\n",
      "Iteration 39 loss 1965.596243\n",
      "Iteration 40 loss 1945.488310\n",
      "Iteration 41 loss 1929.906530\n",
      "Iteration 42 loss 1907.048332\n",
      "Iteration 43 loss 1891.284569\n",
      "Iteration 44 loss 1869.399107\n",
      "Iteration 45 loss 1850.935522\n",
      "Iteration 46 loss 1832.884616\n",
      "Iteration 47 loss 1815.505535\n",
      "Iteration 48 loss 1796.123962\n",
      "Iteration 49 loss 1779.748603\n",
      "Iteration 50 loss 1762.211274\n",
      "Iteration 51 loss 1742.482447\n",
      "Iteration 52 loss 1724.294438\n",
      "Iteration 53 loss 1708.568628\n",
      "Iteration 54 loss 1690.693490\n",
      "Iteration 55 loss 1675.031876\n",
      "Iteration 56 loss 1657.877028\n",
      "Iteration 57 loss 1640.123110\n",
      "Iteration 58 loss 1623.002599\n",
      "Iteration 59 loss 1601.880853\n",
      "Iteration 60 loss 1589.618612\n",
      "Iteration 61 loss 1569.910785\n",
      "Iteration 62 loss 1555.511603\n",
      "Iteration 63 loss 1540.167045\n",
      "Iteration 64 loss 1518.989202\n",
      "Iteration 65 loss 1505.414430\n",
      "Iteration 66 loss 1492.650529\n",
      "Iteration 67 loss 1474.910304\n",
      "Iteration 68 loss 1459.185362\n",
      "Iteration 69 loss 1447.303800\n",
      "Iteration 70 loss 1428.402029\n",
      "Iteration 71 loss 1412.966393\n",
      "Iteration 72 loss 1397.220882\n",
      "Iteration 73 loss 1384.565088\n",
      "Iteration 74 loss 1368.215173\n",
      "Iteration 75 loss 1355.704202\n",
      "Iteration 76 loss 1339.154805\n",
      "Iteration 77 loss 1323.487313\n",
      "Iteration 78 loss 1311.786533\n",
      "Iteration 79 loss 1299.614259\n",
      "Iteration 80 loss 1285.198274\n",
      "Iteration 81 loss 1268.429926\n",
      "Iteration 82 loss 1254.166155\n",
      "Iteration 83 loss 1242.413512\n",
      "Iteration 84 loss 1232.473296\n",
      "Iteration 85 loss 1219.436916\n",
      "Iteration 86 loss 1203.002408\n",
      "Iteration 87 loss 1194.134613\n",
      "Iteration 88 loss 1176.508261\n",
      "Iteration 89 loss 1168.941337\n",
      "Iteration 90 loss 1158.888344\n",
      "Iteration 91 loss 1140.735076\n",
      "Iteration 92 loss 1134.299937\n",
      "Iteration 93 loss 1117.969341\n",
      "Iteration 94 loss 1113.140167\n",
      "Iteration 95 loss 1097.458912\n",
      "Iteration 96 loss 1085.614609\n",
      "Iteration 97 loss 1074.592261\n",
      "Iteration 98 loss 1066.681500\n",
      "Iteration 99 loss 1051.606128\n",
      "Iteration 100 loss 1044.405852\n",
      "Iteration 101 loss 1037.387377\n",
      "Iteration 102 loss 1029.832240\n",
      "Iteration 103 loss 1016.869086\n",
      "Iteration 104 loss 1009.307888\n",
      "Iteration 105 loss 997.637064\n",
      "Iteration 106 loss 985.720728\n",
      "Iteration 107 loss 980.907363\n",
      "Iteration 108 loss 977.741630\n",
      "Iteration 109 loss 962.341443\n",
      "Iteration 110 loss 958.829040\n",
      "Iteration 111 loss 951.679000\n",
      "Iteration 112 loss 955.495738\n"
     ]
    }
   ],
   "source": [
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    return X/np.max(X)\n",
    "\n",
    "XX = normalize(X_train)\n",
    "Xtest = normalize(X_test)\n",
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 150\n",
    "h1 = 200\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "L = len(W)-1\n",
    "\n",
    "def RELU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_der(z):\n",
    "    return 1.*(z>0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def softmax_der(x):\n",
    "    prob = softmax(x)\n",
    "#     print(np.shape(prob))\n",
    "    r = np.multiply(prob, (1-prob))\n",
    "    return r\n",
    "\n",
    "def oneHotEncode(y,softmaxClasses):\n",
    "        yactual = np.matrix(np.zeros(softmaxClasses)).T\n",
    "        yactual[y] = 1\n",
    "        return yactual    \n",
    "    \n",
    "def crossEntropy(ypred,y,softmaxClasses):\n",
    "    #one hot encode\n",
    "    yactual = oneHotEncode(y,softmaxClasses)\n",
    "    l = np.log(ypred)\n",
    "    return np.sum(-1 * l.T * yactual)\n",
    "\n",
    "\n",
    "def delta_cross_entropy(y, X):\n",
    "          m = y.shape[0]\n",
    "          grad = softmax(X)\n",
    "          idx = np.where(grad == y.T*grad)\n",
    "          grad[idx] = grad[idx] - 1\n",
    "          grad = grad/m\n",
    "          return grad\n",
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))\n",
    "    \n",
    "# Use mini-batch size 1\n",
    "# M = 6\n",
    "alpha = 0.002\n",
    "max_iter = 500\n",
    "import math\n",
    "error = math.inf\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    prev_iter_loss = error\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y_train[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if l != L:\n",
    "                a.append(RELU(z[l]))\n",
    "            if l == L:\n",
    "                a.append(softmax(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = crossEntropy( a[L], y_this, 10)\n",
    "#         print(a[L])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step\n",
    "\n",
    "        delta[L] = delta_cross_entropy(oneHotEncode(y_this, 10), a[L])\n",
    "#         print(np.sum(delta[L]))\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                if l == L:\n",
    "#                     WxD = W[l]*delta[l]\n",
    "                    delta[l-1] = np.multiply(softmax_der(z[l-1]), W[l]*delta[l] )\n",
    "#                     print(\"softmax delt: \", np.sum(delta[l-1]))\n",
    "                else:\n",
    "                    delta[l-1] = np.multiply(relu_der(z[l-1]), W[l] * delta[l])\n",
    "#                     print(\"relu delt: \", np.sum(delta[l-1]))\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))\n",
    "    error = loss_this_iter\n",
    "    if loss_this_iter > prev_iter_loss:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.53872053872054\n"
     ]
    }
   ],
   "source": [
    "m = Xtest.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "truePositives = []\n",
    "for i in range(0,m):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = Xtest[order[i],:].T\n",
    "        y_this = y_test[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if l != L:\n",
    "                a.append(RELU(z[l]))\n",
    "            if l == L:\n",
    "                a.append(softmax(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "#         highest = np.where(a[L]>=0.5)\n",
    "        highest = np.argmax(a[L])\n",
    "#         print(np.argmax(a[L]))\n",
    "#         print(highest, \"--- \", y_this)\n",
    "        if highest == y_this:\n",
    "            truePositives.append(True)\n",
    "    \n",
    "accuracy = 100*len(truePositives)/ Xtest.shape[0]\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
